{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"squad\")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "    num_rows: 87599\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets.get(\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f4190066117f',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'What is in front of the Notre Dame Main Building?',\n",
       " 'answers': {'text': ['a copper statue of Christ'], 'answer_start': [188]}}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661180',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?',\n",
       " 'answers': {'text': ['the Main Building'], 'answer_start': [279]}}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['the Main Building'], 'answer_start': [279]}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][2][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'University_of_Notre_Dame'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1][\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is in front of the Notre Dame Main Building?'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['a copper statue of Christ'], 'answer_start': [188]}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for train set, ensure that there's always 1 answer\n",
    "# not multiple answers, or no answers\n",
    "raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Santa Clara, California',\n",
       "  \"Levi's Stadium\",\n",
       "  \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"],\n",
       " 'answer_start': [403, 355, 355]}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for validation set, there may be multiple answers\n",
    "raw_datasets[\"validation\"][2][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why are there multiple answers?\n",
    "raw_datasets[\"validation\"][2][\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# they may even be the same!\n",
    "raw_datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "# model_checkpoint = \"bert-base-cased\" # try it yourself\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] What is in front of the Notre Dame Main Building? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = raw_datasets[\"train\"][1][\"context\"]\n",
    "question = raw_datasets[\"train\"][1][\"question\"]\n",
    "\n",
    "inputs = tokenizer(question, context)\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the G [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernade [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP]rdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# what if the context is really long?\n",
    "# split it into multiple samples\n",
    "inputs = tokenizer(\n",
    "  question,\n",
    "  context,\n",
    "  max_length=100,\n",
    "  truncation=\"only_second\",\n",
    "  stride=50,\n",
    "  return_overflowing_tokens=True,\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "  print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what's the new key?\n",
    "inputs['overflow_to_sample_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:3][\"question\"],\n",
    "    raw_datasets[\"train\"][:3][\"context\"],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "inputs['overflow_to_sample_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1327,\n",
       " 1110,\n",
       " 1107,\n",
       " 1524,\n",
       " 1104,\n",
       " 1103,\n",
       " 10360,\n",
       " 8022,\n",
       " 4304,\n",
       " 4334,\n",
       " 136,\n",
       " 102,\n",
       " 22182,\n",
       " 1193,\n",
       " 117,\n",
       " 1103,\n",
       " 1278,\n",
       " 1144,\n",
       " 170,\n",
       " 2336,\n",
       " 1959,\n",
       " 119,\n",
       " 1335,\n",
       " 4184,\n",
       " 1103,\n",
       " 4304,\n",
       " 4334,\n",
       " 112,\n",
       " 188,\n",
       " 2284,\n",
       " 10945,\n",
       " 1110,\n",
       " 170,\n",
       " 5404,\n",
       " 5921,\n",
       " 1104,\n",
       " 1103,\n",
       " 6567,\n",
       " 2090,\n",
       " 119,\n",
       " 13301,\n",
       " 1107,\n",
       " 1524,\n",
       " 1104,\n",
       " 1103,\n",
       " 4304,\n",
       " 4334,\n",
       " 1105,\n",
       " 4749,\n",
       " 1122,\n",
       " 117,\n",
       " 1110,\n",
       " 170,\n",
       " 7335,\n",
       " 5921,\n",
       " 1104,\n",
       " 4028,\n",
       " 1114,\n",
       " 1739,\n",
       " 1146,\n",
       " 14089,\n",
       " 5591,\n",
       " 1114,\n",
       " 1103,\n",
       " 7051,\n",
       " 107,\n",
       " 159,\n",
       " 21462,\n",
       " 1566,\n",
       " 24930,\n",
       " 2508,\n",
       " 152,\n",
       " 1306,\n",
       " 3965,\n",
       " 107,\n",
       " 119,\n",
       " 5893,\n",
       " 1106,\n",
       " 1103,\n",
       " 4304,\n",
       " 4334,\n",
       " 1110,\n",
       " 1103,\n",
       " 19349,\n",
       " 1104,\n",
       " 1103,\n",
       " 11373,\n",
       " 4641,\n",
       " 119,\n",
       " 13301,\n",
       " 1481,\n",
       " 1103,\n",
       " 171,\n",
       " 17506,\n",
       " 9538,\n",
       " 1110,\n",
       " 1103,\n",
       " 144,\n",
       " 102]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1327,\n",
       " 1110,\n",
       " 1107,\n",
       " 1524,\n",
       " 1104,\n",
       " 1103,\n",
       " 10360,\n",
       " 8022,\n",
       " 4304,\n",
       " 4334,\n",
       " 136,\n",
       " 102,\n",
       " 4749,\n",
       " 1122,\n",
       " 117,\n",
       " 1110,\n",
       " 170,\n",
       " 7335,\n",
       " 5921,\n",
       " 1104,\n",
       " 4028,\n",
       " 1114,\n",
       " 1739,\n",
       " 1146,\n",
       " 14089,\n",
       " 5591,\n",
       " 1114,\n",
       " 1103,\n",
       " 7051,\n",
       " 107,\n",
       " 159,\n",
       " 21462,\n",
       " 1566,\n",
       " 24930,\n",
       " 2508,\n",
       " 152,\n",
       " 1306,\n",
       " 3965,\n",
       " 107,\n",
       " 119,\n",
       " 5893,\n",
       " 1106,\n",
       " 1103,\n",
       " 4304,\n",
       " 4334,\n",
       " 1110,\n",
       " 1103,\n",
       " 19349,\n",
       " 1104,\n",
       " 1103,\n",
       " 11373,\n",
       " 4641,\n",
       " 119,\n",
       " 13301,\n",
       " 1481,\n",
       " 1103,\n",
       " 171,\n",
       " 17506,\n",
       " 9538,\n",
       " 1110,\n",
       " 1103,\n",
       " 144,\n",
       " 10595,\n",
       " 2430,\n",
       " 117,\n",
       " 170,\n",
       " 14789,\n",
       " 1282,\n",
       " 1104,\n",
       " 8070,\n",
       " 1105,\n",
       " 9284,\n",
       " 119,\n",
       " 1135,\n",
       " 1110,\n",
       " 170,\n",
       " 16498,\n",
       " 1104,\n",
       " 1103,\n",
       " 176,\n",
       " 10595,\n",
       " 2430,\n",
       " 1120,\n",
       " 10111,\n",
       " 20500,\n",
       " 117,\n",
       " 1699,\n",
       " 1187,\n",
       " 1103,\n",
       " 6567,\n",
       " 2090,\n",
       " 25153,\n",
       " 1193,\n",
       " 1691,\n",
       " 1106,\n",
       " 2216,\n",
       " 17666,\n",
       " 6397,\n",
       " 102]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the G [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernade [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP]rdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n",
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]\n",
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]\n",
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]\n",
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# it points to the original sample index\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "  print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreate inputs for just a single context-question pair\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0),\n",
       "  (0, 2),\n",
       "  (3, 7),\n",
       "  (8, 11),\n",
       "  (12, 15),\n",
       "  (16, 22),\n",
       "  (23, 27),\n",
       "  (28, 37),\n",
       "  (38, 44),\n",
       "  (45, 47),\n",
       "  (48, 52),\n",
       "  (53, 55),\n",
       "  (56, 59),\n",
       "  (59, 63),\n",
       "  (64, 70),\n",
       "  (70, 71),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 2),\n",
       "  (3, 7),\n",
       "  (8, 11),\n",
       "  (12, 15),\n",
       "  (16, 22),\n",
       "  (23, 27),\n",
       "  (28, 37),\n",
       "  (38, 44),\n",
       "  (45, 47),\n",
       "  (48, 52),\n",
       "  (53, 55),\n",
       "  (56, 59),\n",
       "  (59, 63),\n",
       "  (64, 70),\n",
       "  (70, 71),\n",
       "  (0, 0),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 2),\n",
       "  (3, 7),\n",
       "  (8, 11),\n",
       "  (12, 15),\n",
       "  (16, 22),\n",
       "  (23, 27),\n",
       "  (28, 37),\n",
       "  (38, 44),\n",
       "  (45, 47),\n",
       "  (48, 52),\n",
       "  (53, 55),\n",
       "  (56, 59),\n",
       "  (59, 63),\n",
       "  (64, 70),\n",
       "  (70, 71),\n",
       "  (0, 0),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 2),\n",
       "  (3, 7),\n",
       "  (8, 11),\n",
       "  (12, 15),\n",
       "  (16, 22),\n",
       "  (23, 27),\n",
       "  (28, 37),\n",
       "  (38, 44),\n",
       "  (45, 47),\n",
       "  (48, 52),\n",
       "  (53, 55),\n",
       "  (56, 59),\n",
       "  (59, 63),\n",
       "  (64, 70),\n",
       "  (70, 71),\n",
       "  (0, 0),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 3),\n",
       "  (4, 12),\n",
       "  (13, 15),\n",
       "  (16, 19),\n",
       "  (20, 26),\n",
       "  (27, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 46),\n",
       "  (47, 49),\n",
       "  (50, 56),\n",
       "  (57, 59),\n",
       "  (60, 65),\n",
       "  (66, 75),\n",
       "  (75, 76),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 3),\n",
       "  (4, 12),\n",
       "  (13, 15),\n",
       "  (16, 19),\n",
       "  (20, 26),\n",
       "  (27, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 46),\n",
       "  (47, 49),\n",
       "  (50, 56),\n",
       "  (57, 59),\n",
       "  (60, 65),\n",
       "  (66, 75),\n",
       "  (75, 76),\n",
       "  (0, 0),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 3),\n",
       "  (4, 12),\n",
       "  (13, 15),\n",
       "  (16, 19),\n",
       "  (20, 26),\n",
       "  (27, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 46),\n",
       "  (47, 49),\n",
       "  (50, 56),\n",
       "  (57, 59),\n",
       "  (60, 65),\n",
       "  (66, 75),\n",
       "  (75, 76),\n",
       "  (0, 0),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 3),\n",
       "  (4, 12),\n",
       "  (13, 15),\n",
       "  (16, 19),\n",
       "  (20, 26),\n",
       "  (27, 32),\n",
       "  (33, 35),\n",
       "  (36, 41),\n",
       "  (42, 46),\n",
       "  (47, 49),\n",
       "  (50, 56),\n",
       "  (57, 59),\n",
       "  (60, 65),\n",
       "  (66, 75),\n",
       "  (75, 76),\n",
       "  (0, 0),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)]]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is this (weirdly named) offset_mapping?\n",
    "# it tells us the location of each token\n",
    "# notes:\n",
    "# special tokens take up 0 space - (0, 0)\n",
    "# the question portion is the same for each sample\n",
    "# the context portion starting point inceases in each sample\n",
    "inputs['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs['offset_mapping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs['offset_mapping'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs['offset_mapping'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs['offset_mapping'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs['offset_mapping'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['a copper statue of Christ'], 'answer_start': [188]}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem: the position of the answer will change in each\n",
    "# window of the context\n",
    "# the answer is also the target for the neural network\n",
    "# how can we recompute the targets for each context window?\n",
    "\n",
    "# since we took the question and context from this sample earlier\n",
    "answer = raw_datasets[\"train\"][1][\"answers\"]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs.sequence_ids(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 98)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the start and end of the context (the first and last '1')\n",
    "sequence_ids = inputs.sequence_ids(0)\n",
    "\n",
    "ctx_start = sequence_ids.index(1) # first occurrence\n",
    "ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1 # last occurrence\n",
    "\n",
    "ctx_start, ctx_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 57)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether or not the answer is fully contained within the context\n",
    "# if not, target is (start, end) = (0, 0)\n",
    "\n",
    "ans_start_char = answer['answer_start'][0]\n",
    "ans_end_char = ans_start_char + len(answer['text'][0])\n",
    "\n",
    "offset = inputs['offset_mapping'][0]\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = 0\n",
    "\n",
    "if offset[ctx_start][0] > ans_start_char or offset[ctx_end][1] < ans_end_char:\n",
    "  print(\"target is (0, 0)\")\n",
    "  # nothing else to do\n",
    "else:\n",
    "  # find the start and end TOKEN positions\n",
    "\n",
    "  # the 'trick' is knowing what is in units of tokens and what is in\n",
    "  # units of characters\n",
    "\n",
    "  # recall: the offset_mapping contains the character positions of each token\n",
    "\n",
    "  i = ctx_start\n",
    "  for start_end_char in offset[ctx_start:]:\n",
    "    start, end = start_end_char\n",
    "    if start == ans_start_char:\n",
    "      start_idx = i\n",
    "      # don't break yet\n",
    "    \n",
    "    if end == ans_end_char:\n",
    "      end_idx = i\n",
    "      break\n",
    "\n",
    "    i += 1\n",
    "\n",
    "start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[170, 7335, 5921, 1104, 4028]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "input_ids = inputs['input_ids'][0]\n",
    "input_ids[start_idx : end_idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a copper statue of Christ'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[start_idx : end_idx + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer_token_idx(\n",
    "    ctx_start,\n",
    "    ctx_end,\n",
    "    ans_start_char,\n",
    "    ans_end_char,\n",
    "    offset):\n",
    "  \n",
    "  start_idx = 0\n",
    "  end_idx = 0\n",
    "\n",
    "  if offset[ctx_start][0] > ans_start_char or offset[ctx_end][1] < ans_end_char:\n",
    "    pass\n",
    "    # print(\"target is (0, 0)\")\n",
    "    # nothing else to do\n",
    "  else:\n",
    "    # find the start and end TOKEN positions\n",
    "\n",
    "    # the 'trick' is knowing what is in units of tokens and what is in\n",
    "    # units of characters\n",
    "\n",
    "    # recall: the offset_mapping contains the character positions of each token\n",
    "\n",
    "    i = ctx_start\n",
    "    for start_end_char in offset[ctx_start:]:\n",
    "      start, end = start_end_char\n",
    "      if start == ans_start_char:\n",
    "        start_idx = i\n",
    "        # don't break yet\n",
    "      \n",
    "      if end == ans_end_char:\n",
    "        end_idx = i\n",
    "        break\n",
    "\n",
    "      i += 1\n",
    "  return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([53, 17, 0, 0], [57, 21, 0, 0])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it on all context windows\n",
    "# sometimes, the answer won't appear!\n",
    "\n",
    "start_idxs = []\n",
    "end_idxs = []\n",
    "\n",
    "for i, offset in enumerate(inputs['offset_mapping']):\n",
    "  # the final window may not be full size - can't assume 100\n",
    "  sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "  # find start + end of context (first 1 and last 1)\n",
    "  ctx_start = sequence_ids.index(1)\n",
    "  ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n",
    "\n",
    "  start_idx, end_idx = find_answer_token_idx(\n",
    "    ctx_start,\n",
    "    ctx_end,\n",
    "    ans_start_char,\n",
    "    ans_end_char,\n",
    "    offset)\n",
    "\n",
    "  start_idxs.append(start_idx)\n",
    "  end_idxs.append(end_idx)\n",
    "\n",
    "start_idxs, end_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In what city and state did Beyonce  grow up? \n",
      " The album, Dangerously in Love  achieved what spot on the Billboard Top 100 chart?\n",
      "Which song did Beyonce sing at the first couple's inaugural ball? \n",
      "What event did Beyoncé perform at one month after Obama's inauguration? \n",
      "Where was the album released? \n",
      "What movie influenced Beyonce towards empowerment themes? \n"
     ]
    }
   ],
   "source": [
    "# some questions have leading and/or trailing whitespace\n",
    "for q in raw_datasets[\"train\"][\"question\"][:1000]:\n",
    "  if q.strip() != q:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are ready to process (tokenize) the training data\n",
    "# (i.e. expand question+context pairs into question+smaller context windows)\n",
    "\n",
    "# Google used 384 for SQuAD\n",
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "def tokenize_fn_train(batch):\n",
    "  # some questions have leading and/or trailing whitespace\n",
    "  questions = [q.strip() for q in batch[\"question\"]]\n",
    "\n",
    "  # tokenize the data (with padding this time)\n",
    "  # since most contexts are long, we won't bother to pad per-minibatch\n",
    "  inputs = tokenizer(\n",
    "    questions,\n",
    "    batch[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    stride=stride,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    "  )\n",
    "\n",
    "  # we don't need these later so remove them\n",
    "  offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "  orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "  answers = batch['answers']\n",
    "  start_idxs, end_idxs = [], []\n",
    "\n",
    "  # same loop as above\n",
    "  for i, offset in enumerate(offset_mapping):\n",
    "    sample_idx = orig_sample_idxs[i]\n",
    "    answer = answers[sample_idx]\n",
    "\n",
    "    ans_start_char = answer['answer_start'][0]\n",
    "    ans_end_char = ans_start_char + len(answer['text'][0])\n",
    "\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # find start + end of context (first 1 and last 1)\n",
    "    ctx_start = sequence_ids.index(1)\n",
    "    ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n",
    "\n",
    "    start_idx, end_idx = find_answer_token_idx(\n",
    "      ctx_start,\n",
    "      ctx_end,\n",
    "      ans_start_char,\n",
    "      ans_end_char,\n",
    "      offset)\n",
    "\n",
    "    start_idxs.append(start_idx)\n",
    "    end_idxs.append(end_idx)\n",
    "  \n",
    "  inputs[\"start_positions\"] = start_idxs\n",
    "  inputs[\"end_positions\"] = end_idxs\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 88729)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "  tokenize_fn_train,\n",
    "  batched=True,\n",
    "  remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be4db0acb8001400a502ec',\n",
       " 'title': 'Super_Bowl_50',\n",
       " 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       "  'answer_start': [177, 177, 177]}}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: we'll keep these IDs for later\n",
    "raw_datasets[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the validation set differently\n",
    "# we won't need the targets since we will just compare with the original answer\n",
    "# also: overwrite offset_mapping with Nones in place of question\n",
    "def tokenize_fn_validation(batch):\n",
    "  # some questions have leading and/or trailing whitespace\n",
    "  questions = [q.strip() for q in batch[\"question\"]]\n",
    "\n",
    "  # tokenize the data (with padding this time)\n",
    "  # since most contexts are long, we won't bother to pad per-minibatch\n",
    "  inputs = tokenizer(\n",
    "    questions,\n",
    "    batch[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    stride=stride,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    "  )\n",
    "\n",
    "  # we don't need these later so remove them\n",
    "  orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "  sample_ids = []\n",
    "\n",
    "  # rewrite offset mapping by replacing question tuples with None\n",
    "  # this will be helpful later on when we compute metrics\n",
    "  for i in range(len(inputs[\"input_ids\"])):\n",
    "    sample_idx = orig_sample_idxs[i]\n",
    "    sample_ids.append(batch['id'][sample_idx])\n",
    "\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "    offset = inputs[\"offset_mapping\"][i]\n",
    "    inputs[\"offset_mapping\"][i] = [\n",
    "      x if sequence_ids[j] == 1 else None for j, x in enumerate(offset)]\n",
    "    \n",
    "  inputs['sample_id'] = sample_ids\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10570, 10822)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "  tokenize_fn_validation,\n",
    "  batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 66.66666666666667, 'f1': 83.33333333333333}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_answers = [\n",
    "  {'id': '1', 'prediction_text': 'Albert Einstein'},\n",
    "  {'id': '2', 'prediction_text': 'physicist'},\n",
    "  {'id': '3', 'prediction_text': 'general relativity'},\n",
    "]\n",
    "true_answers = [\n",
    "  {'id': '1', 'answers': {'text': ['Albert Einstein'], 'answer_start': [100]}},\n",
    "  {'id': '2', 'answers': {'text': ['physicist'], 'answer_start': [100]}},\n",
    "  {'id': '3', 'answers': {'text': ['special relativity'], 'answer_start': [100]}},\n",
    "]\n",
    "\n",
    "# id and answer_start seem superfluous but you'll get an error if not included\n",
    "# exercise: remove them (one at a time) and see!\n",
    "metric.compute(predictions=predicted_answers, references=true_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next problem: how to go from logits to prediction text?\n",
    "# to make it easier, let's work on an already-trained question-answering model\n",
    "small_validation_dataset = raw_datasets[\"validation\"].select(range(100))\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "\n",
    "# temporarily assign tokenizer2 to tokenizer since it's used as a global\n",
    "# in tokenize_fn_validation\n",
    "old_tokenizer = tokenizer\n",
    "tokenizer = tokenizer2\n",
    "\n",
    "small_validation_processed = small_validation_dataset.map(\n",
    "    tokenize_fn_validation,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "\n",
    "# change it back\n",
    "tokenizer = old_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nilofar\\Desktop\\Q-Answering_Lazyprogrammer\\QA1.ipynb Cell 48\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#X61sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# get the model outputs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#X61sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#X61sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   outputs \u001b[39m=\u001b[39m trained_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msmall_model_inputs_gpu)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:911\u001b[0m, in \u001b[0;36mDistilBertForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    907\u001b[0m \u001b[39m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    909\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m--> 911\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[0;32m    912\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    913\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    914\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    915\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    916\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    917\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    918\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    919\u001b[0m )\n\u001b[0;32m    920\u001b[0m hidden_states \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, max_query_len, dim)\u001b[39;00m\n\u001b[0;32m    922\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)  \u001b[39m# (bs, max_query_len, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:609\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    605\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    607\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m    610\u001b[0m     x\u001b[39m=\u001b[39;49membeddings,\n\u001b[0;32m    611\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    612\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    613\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    614\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    615\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    616\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:375\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    368\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    369\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    370\u001b[0m         hidden_state,\n\u001b[0;32m    371\u001b[0m         attn_mask,\n\u001b[0;32m    372\u001b[0m         head_mask[i],\n\u001b[0;32m    373\u001b[0m     )\n\u001b[0;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 375\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    376\u001b[0m         hidden_state,\n\u001b[0;32m    377\u001b[0m         attn_mask,\n\u001b[0;32m    378\u001b[0m         head_mask[i],\n\u001b[0;32m    379\u001b[0m         output_attentions,\n\u001b[0;32m    380\u001b[0m     )\n\u001b[0;32m    382\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    384\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    296\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    297\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    298\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    299\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    300\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    301\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    302\u001b[0m )\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    304\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:226\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    221\u001b[0m mask \u001b[39m=\u001b[39m (mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mview(mask_reshp)\u001b[39m.\u001b[39mexpand_as(scores)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    222\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mmasked_fill(\n\u001b[0;32m    223\u001b[0m     mask, torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mfinfo(scores\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin)\n\u001b[0;32m    224\u001b[0m )  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(scores, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    227\u001b[0m weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(weights)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1842\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[0;32m   1844\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get the model outputs\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# the trained model doesn't use these columns\n",
    "small_model_inputs = small_validation_processed.remove_columns(\n",
    "  [\"sample_id\", \"offset_mapping\"])\n",
    "small_model_inputs.set_format(\"torch\")\n",
    "\n",
    "# get gpu device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# move tensors to gpu device\n",
    "small_model_inputs_gpu = {\n",
    "  k: small_model_inputs[k].to(device) for k in small_model_inputs.column_names\n",
    "}\n",
    "\n",
    "# download the model\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "  trained_checkpoint).to(device)\n",
    "\n",
    "# get the model outputs\n",
    "with torch.no_grad():\n",
    "  outputs = trained_model(**small_model_inputs_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ -2.2607,  -5.1783,  -5.2709,  ...,  -9.5243,  -9.5183,  -9.5288],\n",
       "        [ -2.5961,  -5.5482,  -5.5313,  ...,  -9.9598,  -9.9533,  -9.9860],\n",
       "        [ -3.7127,  -7.1848,  -8.5388,  ..., -11.6557, -11.6571, -11.6505],\n",
       "        ...,\n",
       "        [ -2.0260,  -4.4167,  -4.4980,  ...,  -8.1479,  -8.1530,  -8.1760],\n",
       "        [ -4.1553,  -5.8304,  -7.1643,  ..., -10.5255, -10.5251, -10.4890],\n",
       "        [ -3.2000,  -5.8162,  -6.7249,  ...,  -9.4935,  -9.5038,  -9.4871]]), end_logits=tensor([[ -0.7353,  -4.9236,  -5.1048,  ...,  -8.8734,  -8.8915,  -8.8550],\n",
       "        [ -1.3056,  -5.3870,  -5.4945,  ...,  -9.4895,  -9.5039,  -9.4958],\n",
       "        [ -2.7649,  -7.2201,  -9.0916,  ..., -11.3106, -11.3414, -11.2702],\n",
       "        ...,\n",
       "        [ -0.0768,  -4.8210,  -4.4374,  ...,  -8.0483,  -8.0502,  -7.9903],\n",
       "        [ -2.7347,  -5.3650,  -7.2549,  ..., -10.0498, -10.0661,  -9.9886],\n",
       "        [ -1.0991,  -4.2569,  -6.1267,  ...,  -8.6882,  -8.6889,  -8.6272]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['56be4db0acb8001400a502ec',\n",
       " '56be4db0acb8001400a502ed',\n",
       " '56be4db0acb8001400a502ee',\n",
       " '56be4db0acb8001400a502ef',\n",
       " '56be4db0acb8001400a502f0']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_validation_processed['sample_id'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10822"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_dataset['sample_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10570"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(validation_dataset['sample_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: {'56be4db0acb8001400a502ec': [0, 1, 2, 3], ...}\n",
    "sample_id2idxs = {}\n",
    "for i, id_ in enumerate(small_validation_processed['sample_id']):\n",
    "  if id_ not in sample_id2idxs:\n",
    "    sample_id2idxs[id_] = [i]\n",
    "  else:\n",
    "    print(\"here\")\n",
    "    sample_id2idxs[id_].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 384), (100, 384))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits.shape, end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 46,  57,  47,  38,  39,  58,  50,  43,  45,  54,  56,  49,  13,\n",
       "        42,  40,  35,  27,  31,  48,  41,  53,  44,  37,  59,  78,  15,\n",
       "         0,  52,  24,  65,  81,  70,  18,  51,  55,  26,  69,  29,  28,\n",
       "        75,  61,  64,  23,  36,  32,  11, 101,  62,  66,  34,  95,  30,\n",
       "        63,  21,  19,  20,  17,  14,  22,  33,  68,  87, 171,  12,  76,\n",
       "        71,  73,  92, 110,  84, 151,   1,  74,   2,   6,  16,  80,  79,\n",
       "       105,  98,  10,  96, 136, 169, 106, 100,  93, 165,  67, 109,   8,\n",
       "        90,   3, 115,  60,   5,  97,   7, 103, 102,  86,  72, 111,  89,\n",
       "       108,   4,  88,  25, 132,  77, 123, 150, 124, 153,  83, 118,  82,\n",
       "        85, 107, 114, 143, 164, 137, 130, 166, 159, 131,  91,   9, 144,\n",
       "       139, 160,  94, 141, 128, 112, 134, 152, 170, 154, 117, 127, 104,\n",
       "       140, 157, 155, 133, 145, 119, 162, 138, 135, 156, 167, 168, 126,\n",
       "       148, 163, 161, 116,  99, 120, 142, 158, 125, 146, 113, 121, 147,\n",
       "       149, 129, 122, 311, 312, 304, 309, 313, 310, 300, 307, 316, 308,\n",
       "       314, 306, 317, 320, 319, 321, 291, 318, 301, 305, 287, 270, 315,\n",
       "       295, 289, 294, 251, 333, 303, 269, 299, 274, 265, 298, 176, 175,\n",
       "       338, 292, 323, 322, 290, 252, 296, 229, 177, 302, 297, 186, 245,\n",
       "       250, 283, 174, 256, 337, 266, 190, 293, 286, 264, 288, 331, 327,\n",
       "       234, 237, 227, 284, 255, 326, 276, 272, 233, 346, 191, 230, 218,\n",
       "       232, 179, 285, 273, 173, 187, 239, 332, 172, 267, 329, 238, 253,\n",
       "       334, 214, 192, 325, 278, 350, 259, 281, 268, 185, 254, 271, 279,\n",
       "       342, 345, 343, 181, 335, 183, 189, 260, 341, 275, 178, 228, 210,\n",
       "       324, 277, 212, 348, 209, 336, 261, 240, 249, 246, 194, 257, 182,\n",
       "       377, 258, 196, 195, 248, 188, 339, 197, 213, 378, 263, 208, 201,\n",
       "       205, 200, 225, 211, 282, 236, 204, 347, 203, 262, 223, 193, 330,\n",
       "       199, 202, 349, 184, 180, 351, 340, 226, 224, 243, 217, 372, 244,\n",
       "       241, 207, 235, 344, 215, 367, 247, 368, 382, 352, 379, 353, 221,\n",
       "       376, 231, 380, 220, 371, 366, 242, 219, 381, 206, 375, 369, 216,\n",
       "       198, 355, 383, 280, 328, 222, 358, 373, 357, 363, 356, 374, 354,\n",
       "       359, 365, 370, 364, 362, 361, 360], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reminder of how to find indices with the largest values\n",
    "(-start_logits[0]).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.694451  ,  9.803686  ,  4.4599786 ,  4.400489  ,  2.9437866 ,\n",
       "        2.7017422 ,  2.012652  ,  1.5780808 ,  0.52237856,  0.02074176,\n",
       "       -0.02801922, -0.04970397, -0.38572356, -0.6945309 , -0.7979434 ,\n",
       "       -0.86779773, -0.8722001 , -1.3516843 , -1.3703636 , -1.3878787 ,\n",
       "       -1.5135026 , -1.7355411 , -1.8827026 , -1.8932847 , -1.9078901 ,\n",
       "       -1.9304947 , -2.2607286 , -2.2983828 , -2.3069277 , -2.5027347 ,\n",
       "       -2.5100536 , -2.5308342 , -2.5399885 , -2.6718073 , -2.7323494 ,\n",
       "       -2.7710114 , -2.7713616 , -2.952129  , -3.0604606 , -3.170597  ,\n",
       "       -3.2045417 , -3.569333  , -3.5797982 , -3.6668777 , -3.7250547 ,\n",
       "       -3.7498534 , -3.7632098 , -3.9968114 , -4.011324  , -4.0687957 ,\n",
       "       -4.0944786 , -4.19547   , -4.238307  , -4.332359  , -4.35241   ,\n",
       "       -4.3879576 , -4.388604  , -4.396608  , -4.6790495 , -4.7030234 ,\n",
       "       -4.77575   , -4.777807  , -4.7882137 , -4.7882376 , -4.822118  ,\n",
       "       -4.8725348 , -4.884929  , -4.898141  , -5.072089  , -5.107869  ,\n",
       "       -5.148633  , -5.178324  , -5.191201  , -5.270891  , -5.3146367 ,\n",
       "       -5.3770666 , -5.5316434 , -5.5711217 , -5.649811  , -5.6613007 ,\n",
       "       -5.6778693 , -5.698604  , -5.7515626 , -5.842641  , -5.862148  ,\n",
       "       -5.9076815 , -5.9093714 , -5.944444  , -5.9968905 , -6.0058203 ,\n",
       "       -6.0470276 , -6.0640063 , -6.0858717 , -6.1005545 , -6.224171  ,\n",
       "       -6.267088  , -6.2752852 , -6.3032913 , -6.3134108 , -6.3287973 ,\n",
       "       -6.330685  , -6.4014792 , -6.4204807 , -6.436103  , -6.4374037 ,\n",
       "       -6.4507074 , -6.5314255 , -6.5530386 , -6.563015  , -6.66879   ,\n",
       "       -6.7266874 , -6.742626  , -6.7442894 , -6.7458506 , -6.810868  ,\n",
       "       -6.921628  , -6.949293  , -6.990761  , -6.998742  , -7.162259  ,\n",
       "       -7.1673503 , -7.175652  , -7.180557  , -7.1967764 , -7.2435923 ,\n",
       "       -7.2733355 , -7.2769456 , -7.300214  , -7.3009276 , -7.3218546 ,\n",
       "       -7.3976755 , -7.4330273 , -7.5248423 , -7.5272284 , -7.532271  ,\n",
       "       -7.5737076 , -7.59821   , -7.6580915 , -7.6597023 , -7.7155733 ,\n",
       "       -7.7183986 , -7.740014  , -7.7546453 , -7.885642  , -7.893759  ,\n",
       "       -7.8943686 , -7.9380317 , -7.9581985 , -8.03751   , -8.059216  ,\n",
       "       -8.063906  , -8.104628  , -8.117329  , -8.138481  , -8.145773  ,\n",
       "       -8.221216  , -8.274244  , -8.292221  , -8.293204  , -8.316826  ,\n",
       "       -8.317493  , -8.333995  , -8.466507  , -8.500577  , -8.502031  ,\n",
       "       -8.591032  , -8.612466  , -8.621489  , -8.625208  , -8.625452  ,\n",
       "       -8.814257  , -9.039049  , -9.421054  , -9.434788  , -9.441465  ,\n",
       "       -9.44437   , -9.449843  , -9.450167  , -9.450434  , -9.45104   ,\n",
       "       -9.452059  , -9.452422  , -9.452511  , -9.454128  , -9.456524  ,\n",
       "       -9.458102  , -9.458338  , -9.458435  , -9.45875   , -9.460867  ,\n",
       "       -9.4620695 , -9.462508  , -9.46294   , -9.463791  , -9.463888  ,\n",
       "       -9.465527  , -9.466446  , -9.468559  , -9.468852  , -9.468952  ,\n",
       "       -9.469486  , -9.469515  , -9.47006   , -9.470961  , -9.471134  ,\n",
       "       -9.471281  , -9.471354  , -9.4717865 , -9.4718895 , -9.472044  ,\n",
       "       -9.472683  , -9.472958  , -9.473705  , -9.475197  , -9.476604  ,\n",
       "       -9.476763  , -9.47678   , -9.477406  , -9.478306  , -9.478869  ,\n",
       "       -9.479746  , -9.479916  , -9.480066  , -9.480648  , -9.481186  ,\n",
       "       -9.481466  , -9.481517  , -9.4815235 , -9.481607  , -9.481789  ,\n",
       "       -9.482155  , -9.482843  , -9.48341   , -9.4845705 , -9.484707  ,\n",
       "       -9.484895  , -9.485168  , -9.485298  , -9.485716  , -9.485956  ,\n",
       "       -9.486801  , -9.486967  , -9.487631  , -9.487726  , -9.487911  ,\n",
       "       -9.487989  , -9.4882965 , -9.488396  , -9.48859   , -9.488799  ,\n",
       "       -9.488955  , -9.489097  , -9.489484  , -9.489672  , -9.489996  ,\n",
       "       -9.490801  , -9.4908285 , -9.4908695 , -9.490896  , -9.491137  ,\n",
       "       -9.491253  , -9.491707  , -9.492024  , -9.492165  , -9.492535  ,\n",
       "       -9.492814  , -9.493554  , -9.493632  , -9.493645  , -9.494001  ,\n",
       "       -9.494024  , -9.494388  , -9.495201  , -9.49542   , -9.496533  ,\n",
       "       -9.4966    , -9.497129  , -9.497523  , -9.497554  , -9.497576  ,\n",
       "       -9.497602  , -9.497613  , -9.4980135 , -9.498119  , -9.498622  ,\n",
       "       -9.49864   , -9.498762  , -9.499582  , -9.50001   , -9.50034   ,\n",
       "       -9.500418  , -9.500574  , -9.500973  , -9.500995  , -9.501165  ,\n",
       "       -9.501216  , -9.50127   , -9.50143   , -9.501499  , -9.501879  ,\n",
       "       -9.502006  , -9.502519  , -9.502683  , -9.503559  , -9.5036335 ,\n",
       "       -9.503656  , -9.504164  , -9.504545  , -9.505045  , -9.505529  ,\n",
       "       -9.506167  , -9.506636  , -9.50667   , -9.506752  , -9.506826  ,\n",
       "       -9.507116  , -9.507551  , -9.507726  , -9.507986  , -9.508047  ,\n",
       "       -9.50808   , -9.508731  , -9.508766  , -9.50886   , -9.509272  ,\n",
       "       -9.50951   , -9.509527  , -9.51082   , -9.511486  , -9.512029  ,\n",
       "       -9.512419  , -9.512815  , -9.51313   , -9.513149  , -9.513187  ,\n",
       "       -9.513235  , -9.513384  , -9.514926  , -9.5154295 , -9.515565  ,\n",
       "       -9.516184  , -9.516241  , -9.516339  , -9.516375  , -9.516655  ,\n",
       "       -9.517014  , -9.518305  , -9.518465  , -9.518789  , -9.519087  ,\n",
       "       -9.520797  , -9.520998  , -9.521187  , -9.521225  , -9.522537  ,\n",
       "       -9.5227375 , -9.522882  , -9.5232315 , -9.523891  , -9.524325  ,\n",
       "       -9.525372  , -9.526033  , -9.527027  , -9.528467  , -9.528528  ,\n",
       "       -9.528619  , -9.528757  , -9.529456  , -9.529707  , -9.531367  ,\n",
       "       -9.532364  , -9.532742  , -9.535534  , -9.536642  , -9.5377035 ,\n",
       "       -9.53866   , -9.539051  , -9.539336  , -9.540269  , -9.541702  ,\n",
       "       -9.548321  , -9.554476  , -9.557684  , -9.567398  ], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits[0][(-start_logits[0]).argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " [0, 5],\n",
       " [6, 10],\n",
       " [11, 13],\n",
       " [14, 17],\n",
       " [18, 20],\n",
       " [21, 29],\n",
       " [30, 38],\n",
       " [39, 43],\n",
       " [44, 46],\n",
       " [47, 56],\n",
       " [57, 60],\n",
       " [61, 69],\n",
       " [70, 72],\n",
       " [73, 76],\n",
       " [77, 85],\n",
       " [86, 94],\n",
       " [95, 101],\n",
       " [102, 103],\n",
       " [103, 106],\n",
       " [106, 107],\n",
       " [108, 111],\n",
       " [112, 115],\n",
       " [116, 120],\n",
       " [121, 127],\n",
       " [127, 128],\n",
       " [129, 132],\n",
       " [133, 141],\n",
       " [142, 150],\n",
       " [151, 161],\n",
       " [162, 163],\n",
       " [163, 166],\n",
       " [166, 167],\n",
       " [168, 176],\n",
       " [177, 183],\n",
       " [184, 191],\n",
       " [192, 200],\n",
       " [201, 204],\n",
       " [205, 213],\n",
       " [214, 222],\n",
       " [223, 233],\n",
       " [234, 235],\n",
       " [235, 238],\n",
       " [238, 239],\n",
       " [240, 248],\n",
       " [249, 257],\n",
       " [258, 266],\n",
       " [267, 269],\n",
       " [269, 270],\n",
       " [270, 272],\n",
       " [273, 275],\n",
       " [276, 280],\n",
       " [281, 286],\n",
       " [287, 292],\n",
       " [293, 298],\n",
       " [299, 303],\n",
       " [304, 309],\n",
       " [309, 310],\n",
       " [311, 314],\n",
       " [315, 319],\n",
       " [320, 323],\n",
       " [324, 330],\n",
       " [331, 333],\n",
       " [334, 342],\n",
       " [343, 344],\n",
       " [344, 345],\n",
       " [346, 350],\n",
       " [350, 351],\n",
       " [352, 354],\n",
       " [355, 359],\n",
       " [359, 360],\n",
       " [360, 361],\n",
       " [362, 369],\n",
       " [370, 372],\n",
       " [373, 376],\n",
       " [377, 380],\n",
       " [381, 390],\n",
       " [391, 394],\n",
       " [395, 399],\n",
       " [400, 402],\n",
       " [403, 408],\n",
       " [409, 414],\n",
       " [414, 415],\n",
       " [416, 426],\n",
       " [426, 427],\n",
       " [428, 430],\n",
       " [431, 435],\n",
       " [436, 439],\n",
       " [440, 443],\n",
       " [444, 448],\n",
       " [449, 454],\n",
       " [455, 459],\n",
       " [459, 460],\n",
       " [461, 464],\n",
       " [465, 471],\n",
       " [472, 482],\n",
       " [483, 486],\n",
       " [487, 488],\n",
       " [488, 494],\n",
       " [495, 506],\n",
       " [506, 507],\n",
       " [508, 512],\n",
       " [513, 520],\n",
       " [521, 525],\n",
       " [525, 526],\n",
       " [526, 532],\n",
       " [533, 544],\n",
       " [544, 545],\n",
       " [546, 548],\n",
       " [549, 553],\n",
       " [554, 556],\n",
       " [557, 568],\n",
       " [569, 571],\n",
       " [571, 573],\n",
       " [573, 579],\n",
       " [580, 583],\n",
       " [584, 593],\n",
       " [594, 596],\n",
       " [597, 603],\n",
       " [604, 608],\n",
       " [609, 614],\n",
       " [615, 619],\n",
       " [620, 624],\n",
       " [625, 629],\n",
       " [630, 635],\n",
       " [636, 637],\n",
       " [637, 640],\n",
       " [640, 644],\n",
       " [645, 646],\n",
       " [646, 651],\n",
       " [652, 657],\n",
       " [658, 661],\n",
       " [662, 666],\n",
       " [667, 672],\n",
       " [673, 677],\n",
       " [678, 682],\n",
       " [683, 688],\n",
       " [689, 691],\n",
       " [692, 693],\n",
       " [693, 698],\n",
       " [699, 703],\n",
       " [704, 705],\n",
       " [705, 706],\n",
       " [706, 707],\n",
       " [707, 708],\n",
       " [709, 711],\n",
       " [712, 716],\n",
       " [717, 720],\n",
       " [721, 725],\n",
       " [726, 731],\n",
       " [732, 743],\n",
       " [744, 751],\n",
       " [752, 755],\n",
       " [756, 762],\n",
       " [763, 764],\n",
       " [764, 767],\n",
       " [767, 771],\n",
       " [772, 774],\n",
       " [774, 775],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reminder: in offset_mapping we store None everywhere except the context window\n",
    "# in the context window we store tuples for each token containing:\n",
    "# (start_character_position, end_character_position)\n",
    "small_validation_processed['offset_mapping'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_largest = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "# we are looping through the original (untokenized) dataset\n",
    "# because we need to grab the answer from the original string context\n",
    "for sample in small_validation_dataset:\n",
    "  sample_id = sample['id']\n",
    "  context = sample['context']\n",
    "\n",
    "  # update these as we loop through candidate answers\n",
    "  best_score = float('-inf')\n",
    "  best_answer = None\n",
    "\n",
    "  # now loop through the *expanded* input samples (fixed size context windows)\n",
    "  # from here we will pick the highest probability start/end combination\n",
    "  for idx in sample_id2idxs[sample_id]:\n",
    "    start_logit = start_logits[idx] # (384,) vector\n",
    "    end_logit = end_logits[idx] # (384,) vector\n",
    "    offsets = small_validation_processed[idx]['offset_mapping']\n",
    "\n",
    "    start_indices = (-start_logit).argsort()\n",
    "    end_indices = (-end_logit).argsort()\n",
    "\n",
    "    for start_idx in start_indices[:n_largest]:\n",
    "      for end_idx in end_indices[:n_largest]:\n",
    "\n",
    "        # skip answers not contained in context window\n",
    "        # recall: we set entries not pertaining to context to None earlier\n",
    "        if offsets[start_idx] is None or offsets[end_idx] is None:\n",
    "          continue\n",
    "        \n",
    "        # skip answers where end < start\n",
    "        if end_idx < start_idx:\n",
    "          continue\n",
    "        \n",
    "        # skip answers that are too long\n",
    "        if end_idx - start_idx + 1 > max_answer_length:\n",
    "          continue\n",
    "        \n",
    "        # see theory lecture for score calculation\n",
    "        score = start_logit[start_idx] + end_logit[end_idx]\n",
    "        if score > best_score:\n",
    "          best_score = score\n",
    "\n",
    "          # find positions of start and end characters\n",
    "          # recall: offsets contains tuples for each token:\n",
    "          # (start_char, end_char)\n",
    "          first_ch = offsets[start_idx][0]\n",
    "          last_ch = offsets[end_idx][1]\n",
    "\n",
    "          best_answer = context[first_ch:last_ch]\n",
    "\n",
    "  # save best answer\n",
    "  predicted_answers.append({'id': sample_id, 'prediction_text': best_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_validation_dataset['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now test it!\n",
    "\n",
    "true_answers = [\n",
    "  {'id': x['id'], 'answers': x['answers']} for x in small_validation_dataset\n",
    "]\n",
    "metric.compute(predictions=predicted_answers, references=true_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's define a full compute_metrics function\n",
    "# note: this will NOT be called from the trainer\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, processed_dataset, orig_dataset):\n",
    "  # map sample_id ('56be4db0acb8001400a502ec') to row indices of processed data\n",
    "  sample_id2idxs = {}\n",
    "  for i, id_ in enumerate(processed_dataset['sample_id']):\n",
    "    if id_ not in sample_id2idxs:\n",
    "      sample_id2idxs[id_] = [i]\n",
    "    else:\n",
    "      sample_id2idxs[id_].append(i)\n",
    "\n",
    "  predicted_answers = []\n",
    "  for sample in tqdm(orig_dataset):\n",
    "\n",
    "    sample_id = sample['id']\n",
    "    context = sample['context']\n",
    "\n",
    "    # update these as we loop through candidate answers\n",
    "    best_score = float('-inf')\n",
    "    best_answer = None\n",
    "\n",
    "    # now loop through the *expanded* input samples (fixed size context windows)\n",
    "    # from here we will pick the highest probability start/end combination\n",
    "    for idx in sample_id2idxs[sample_id]:\n",
    "      start_logit = start_logits[idx] # (T,) vector\n",
    "      end_logit = end_logits[idx] # (T,) vector\n",
    "\n",
    "      # note: do NOT do the reverse: ['offset_mapping'][idx]\n",
    "      offsets = processed_dataset[idx]['offset_mapping']\n",
    "\n",
    "      start_indices = (-start_logit).argsort()\n",
    "      end_indices = (-end_logit).argsort()\n",
    "\n",
    "      for start_idx in start_indices[:n_largest]:\n",
    "        for end_idx in end_indices[:n_largest]:\n",
    "\n",
    "          # skip answers not contained in context window\n",
    "          # recall: we set entries not pertaining to context to None earlier\n",
    "          if offsets[start_idx] is None or offsets[end_idx] is None:\n",
    "            continue\n",
    "          \n",
    "          # skip answers where end < start\n",
    "          if end_idx < start_idx:\n",
    "            continue\n",
    "          \n",
    "          # skip answers that are too long\n",
    "          if end_idx - start_idx + 1 > max_answer_length:\n",
    "            continue\n",
    "          \n",
    "          # see theory lecture for score calculation\n",
    "          score = start_logit[start_idx] + end_logit[end_idx]\n",
    "          if score > best_score:\n",
    "            best_score = score\n",
    "\n",
    "            # find positions of start and end characters\n",
    "            # recall: offsets contains tuples for each token:\n",
    "            # (start_char, end_char)\n",
    "            first_ch = offsets[start_idx][0]\n",
    "            last_ch = offsets[end_idx][1]\n",
    "\n",
    "            best_answer = context[first_ch:last_ch]\n",
    "\n",
    "    # save best answer\n",
    "    predicted_answers.append({'id': sample_id, 'prediction_text': best_answer})\n",
    "  \n",
    "  # compute the metrics\n",
    "  true_answers = [\n",
    "    {'id': x['id'], 'answers': x['answers']} for x in orig_dataset\n",
    "  ]\n",
    "  return metric.compute(predictions=predicted_answers, references=true_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 657.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run our function on the same mini dataset as before\n",
    "compute_metrics(\n",
    "    start_logits,\n",
    "    end_logits,\n",
    "    small_validation_processed,\n",
    "    small_validation_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# now load the model we want to fine-tune\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nilofar\\Desktop\\Q-Answering_Lazyprogrammer\\QA1.ipynb Cell 62\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#Y115sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m TrainingArguments\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#Y115sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#Y115sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mfinetuned-squad\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#Y115sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mno\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#Y115sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     save_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#Y115sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m2e-5\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#Y115sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#Y115sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#Y115sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     fp16\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1.ipynb#Y115sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[1;32m<string>:114\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1405\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1399\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[0;32m   1400\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1402\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1403\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1405\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1406\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1407\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1408\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1409\u001b[0m ):\n\u001b[0;32m   1410\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1411\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1412\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA or NPU devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1413\u001b[0m     )\n\u001b[0;32m   1415\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1416\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1417\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1422\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1423\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1852\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1848\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1850\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 1852\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[0;32m     55\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1767\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1765\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available(min_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m0.20.1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   1768\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1769\u001b[0m         )\n\u001b[0;32m   1770\u001b[0m     AcceleratorState\u001b[39m.\u001b[39m_reset_state(reset_partial_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1771\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"finetuned-squad\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
