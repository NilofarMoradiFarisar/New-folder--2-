{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nilofar\\Desktop\\Q-Answering_Lazyprogrammer\\QA1 copy.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m raw_datasets \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39msquad\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m raw_datasets\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\datasets\\load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   2124\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[0;32m   2125\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[0;32m   2126\u001b[0m )\n\u001b[0;32m   2128\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 2129\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   2130\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m   2131\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   2132\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[0;32m   2133\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[0;32m   2134\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   2135\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   2136\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   2137\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   2138\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   2139\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   2140\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   2141\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[0;32m   2142\u001b[0m )\n\u001b[0;32m   2144\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   2145\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\datasets\\load.py:1815\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1813\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[0;32m   1814\u001b[0m     download_config\u001b[39m.\u001b[39mstorage_options\u001b[39m.\u001b[39mupdate(storage_options)\n\u001b[1;32m-> 1815\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[0;32m   1816\u001b[0m     path,\n\u001b[0;32m   1817\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   1818\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1819\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1820\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[0;32m   1821\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[0;32m   1822\u001b[0m )\n\u001b[0;32m   1823\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[0;32m   1824\u001b[0m builder_kwargs \u001b[39m=\u001b[39m dataset_module\u001b[39m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\datasets\\load.py:1487\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[1;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[0;32m   1479\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m   1480\u001b[0m \u001b[39mif\u001b[39;00m filename \u001b[39min\u001b[39;00m [sibling\u001b[39m.\u001b[39mrfilename \u001b[39mfor\u001b[39;00m sibling \u001b[39min\u001b[39;00m dataset_info\u001b[39m.\u001b[39msiblings]:\n\u001b[0;32m   1481\u001b[0m     \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[0;32m   1482\u001b[0m         path,\n\u001b[0;32m   1483\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   1484\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1485\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1486\u001b[0m         dynamic_modules_path\u001b[39m=\u001b[39;49mdynamic_modules_path,\n\u001b[1;32m-> 1487\u001b[0m     )\u001b[39m.\u001b[39;49mget_module()\n\u001b[0;32m   1488\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1489\u001b[0m     \u001b[39mreturn\u001b[39;00m HubDatasetModuleFactoryWithoutScript(\n\u001b[0;32m   1490\u001b[0m         path,\n\u001b[0;32m   1491\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1495\u001b[0m         download_mode\u001b[39m=\u001b[39mdownload_mode,\n\u001b[0;32m   1496\u001b[0m     )\u001b[39m.\u001b[39mget_module()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\datasets\\load.py:1198\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithScript.get_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_module\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DatasetModule:\n\u001b[0;32m   1197\u001b[0m     \u001b[39m# get script and other files\u001b[39;00m\n\u001b[1;32m-> 1198\u001b[0m     local_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_loading_script()\n\u001b[0;32m   1199\u001b[0m     dataset_infos_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_dataset_infos_file()\n\u001b[0;32m   1200\u001b[0m     dataset_readme_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload_dataset_readme_file()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\datasets\\load.py:1166\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithScript.download_loading_script\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[39mif\u001b[39;00m download_config\u001b[39m.\u001b[39mdownload_desc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     download_config\u001b[39m.\u001b[39mdownload_desc \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDownloading builder script\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1166\u001b[0m \u001b[39mreturn\u001b[39;00m cached_path(file_path, download_config\u001b[39m=\u001b[39;49mdownload_config)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\datasets\\utils\\file_utils.py:182\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[0;32m    180\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m    181\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[0;32m    183\u001b[0m         url_or_filename,\n\u001b[0;32m    184\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    185\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[0;32m    186\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[0;32m    187\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[0;32m    188\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[0;32m    189\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[0;32m    190\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[0;32m    191\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    192\u001b[0m         token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mtoken,\n\u001b[0;32m    193\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[0;32m    194\u001b[0m         storage_options\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    195\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    197\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    198\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\datasets\\utils\\file_utils.py:542\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, use_auth_token, ignore_url_params, storage_options, download_desc)\u001b[0m\n\u001b[0;32m    540\u001b[0m     connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     response \u001b[39m=\u001b[39m http_head(\n\u001b[0;32m    543\u001b[0m         url,\n\u001b[0;32m    544\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    545\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    546\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[0;32m    547\u001b[0m         max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[0;32m    548\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    549\u001b[0m     )\n\u001b[0;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:  \u001b[39m# ok\u001b[39;00m\n\u001b[0;32m    551\u001b[0m         etag \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mETag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m use_etag \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\datasets\\utils\\file_utils.py:429\u001b[0m, in \u001b[0;36mhttp_head\u001b[1;34m(url, proxies, headers, cookies, allow_redirects, timeout, max_retries)\u001b[0m\n\u001b[0;32m    427\u001b[0m headers \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(headers) \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m    428\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m get_datasets_user_agent(user_agent\u001b[39m=\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m--> 429\u001b[0m response \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[0;32m    430\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    431\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    432\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    433\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    434\u001b[0m     cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[0;32m    435\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49mallow_redirects,\n\u001b[0;32m    436\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    437\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[0;32m    438\u001b[0m )\n\u001b[0;32m    439\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\datasets\\utils\\file_utils.py:328\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[0;32m    326\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    327\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(), url\u001b[39m=\u001b[39;49murl, timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m    329\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    468\u001b[0m     \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    469\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1092\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[1;32m-> 1092\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1094\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[0;32m   1095\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1096\u001b[0m         (\n\u001b[0;32m   1097\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mconn\u001b[39m.\u001b[39mhost\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1102\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1103\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    610\u001b[0m     sock: socket\u001b[39m.\u001b[39msocket \u001b[39m|\u001b[39m ssl\u001b[39m.\u001b[39mSSLSocket\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    612\u001b[0m     server_hostname: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[0;32m    613\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \n\u001b[0;32m    200\u001b[0m \u001b[39m:return: New socket connection.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[0;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    210\u001b[0m     \u001b[39mraise\u001b[39;00m NameResolutionError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m source_address:\n\u001b[0;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39mconnect(sa)\n\u001b[0;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m     75\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"squad\")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for train set, ensure that there's always 1 answer\n",
    "# not multiple answers, or no answers\n",
    "raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "# model_checkpoint = \"bert-base-cased\" # try it yourself\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] What is in front of the Notre Dame Main Building? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = raw_datasets[\"train\"][1][\"context\"]\n",
    "question = raw_datasets[\"train\"][1][\"question\"]\n",
    "\n",
    "inputs = tokenizer(question, context)\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the G [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernade [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP]rdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# what if the context is really long?\n",
    "# split it into multiple samples\n",
    "inputs = tokenizer(\n",
    "  question,\n",
    "  context,\n",
    "  max_length=100,\n",
    "  truncation=\"only_second\",\n",
    "  stride=50,\n",
    "  return_overflowing_tokens=True,\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "  print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what's the new key?\n",
    "inputs['overflow_to_sample_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:3][\"question\"],\n",
    "    raw_datasets[\"train\"][:3][\"context\"],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "inputs['overflow_to_sample_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]\n",
      "[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the G [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernade [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP] of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern [SEP]\n",
      "[CLS] What is in front of the Notre Dame Main Building? [SEP]rdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n",
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]\n",
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]\n",
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]\n",
      "[CLS] The Basilica of the Sacred heart at Notre Dame is beside to which structure? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# it points to the original sample index\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "  print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreate inputs for just a single context-question pair\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is this (weirdly named) offset_mapping?\n",
    "# it tells us the location of each token\n",
    "# notes:\n",
    "# special tokens take up 0 space - (0, 0)\n",
    "# the question portion is the same for each sample\n",
    "# the context portion starting point inceases in each sample\n",
    "inputs['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['a copper statue of Christ'], 'answer_start': [188]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem: the position of the answer will change in each\n",
    "# window of the context\n",
    "# the answer is also the target for the neural network\n",
    "# how can we recompute the targets for each context window?\n",
    "\n",
    "# since we took the question and context from this sample earlier\n",
    "answer = raw_datasets[\"train\"][1][\"answers\"]\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 98)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the start and end of the context (the first and last '1')\n",
    "sequence_ids = inputs.sequence_ids(0)\n",
    "\n",
    "ctx_start = sequence_ids.index(1) # first occurrence\n",
    "ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1 # last occurrence\n",
    "\n",
    "ctx_start, ctx_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 57)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether or not the answer is fully contained within the context\n",
    "# if not, target is (start, end) = (0, 0)\n",
    "\n",
    "ans_start_char = answer['answer_start'][0]\n",
    "ans_end_char = ans_start_char + len(answer['text'][0])\n",
    "\n",
    "offset = inputs['offset_mapping'][0]\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = 0\n",
    "\n",
    "if offset[ctx_start][0] > ans_start_char or offset[ctx_end][1] < ans_end_char:\n",
    "  print(\"target is (0, 0)\")\n",
    "  # nothing else to do\n",
    "else:\n",
    "  # find the start and end TOKEN positions\n",
    "\n",
    "  # the 'trick' is knowing what is in units of tokens and what is in\n",
    "  # units of characters\n",
    "\n",
    "  # recall: the offset_mapping contains the character positions of each token\n",
    "\n",
    "  i = ctx_start\n",
    "  for start_end_char in offset[ctx_start:]:\n",
    "    start, end = start_end_char\n",
    "    if start == ans_start_char:\n",
    "      start_idx = i\n",
    "      # don't break yet\n",
    "    \n",
    "    if end == ans_end_char:\n",
    "      end_idx = i\n",
    "      break\n",
    "\n",
    "    i += 1\n",
    "\n",
    "start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 4),\n",
       " (5, 7),\n",
       " (8, 10),\n",
       " (11, 16),\n",
       " (17, 19),\n",
       " (20, 23),\n",
       " (24, 29),\n",
       " (30, 34),\n",
       " (35, 39),\n",
       " (40, 48),\n",
       " (48, 49),\n",
       " (0, 0),\n",
       " (0, 13),\n",
       " (13, 15),\n",
       " (15, 16),\n",
       " (17, 20),\n",
       " (21, 27),\n",
       " (28, 31),\n",
       " (32, 33),\n",
       " (34, 42),\n",
       " (43, 52),\n",
       " (52, 53),\n",
       " (54, 56),\n",
       " (56, 58),\n",
       " (59, 62),\n",
       " (63, 67),\n",
       " (68, 76),\n",
       " (76, 77),\n",
       " (77, 78),\n",
       " (79, 83),\n",
       " (84, 88),\n",
       " (89, 91),\n",
       " (92, 93),\n",
       " (94, 100),\n",
       " (101, 107),\n",
       " (108, 110),\n",
       " (111, 114),\n",
       " (115, 121),\n",
       " (122, 126),\n",
       " (126, 127),\n",
       " (128, 139),\n",
       " (140, 142),\n",
       " (143, 148),\n",
       " (149, 151),\n",
       " (152, 155),\n",
       " (156, 160),\n",
       " (161, 169),\n",
       " (170, 173),\n",
       " (174, 180),\n",
       " (181, 183),\n",
       " (183, 184),\n",
       " (185, 187),\n",
       " (188, 189),\n",
       " (190, 196),\n",
       " (197, 203),\n",
       " (204, 206),\n",
       " (207, 213),\n",
       " (214, 218),\n",
       " (219, 223),\n",
       " (224, 226),\n",
       " (226, 229),\n",
       " (229, 232),\n",
       " (233, 237),\n",
       " (238, 241),\n",
       " (242, 248),\n",
       " (249, 250),\n",
       " (250, 251),\n",
       " (251, 254),\n",
       " (254, 256),\n",
       " (257, 259),\n",
       " (260, 262),\n",
       " (263, 264),\n",
       " (264, 265),\n",
       " (265, 268),\n",
       " (268, 269),\n",
       " (269, 270),\n",
       " (271, 275),\n",
       " (276, 278),\n",
       " (279, 282),\n",
       " (283, 287),\n",
       " (288, 296),\n",
       " (297, 299),\n",
       " (300, 303),\n",
       " (304, 312),\n",
       " (313, 315),\n",
       " (316, 319),\n",
       " (320, 326),\n",
       " (327, 332),\n",
       " (332, 333),\n",
       " (334, 345),\n",
       " (346, 352),\n",
       " (353, 356),\n",
       " (357, 358),\n",
       " (358, 361),\n",
       " (361, 365),\n",
       " (366, 368),\n",
       " (369, 372),\n",
       " (373, 374),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['offset_mapping'][0]  # offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[170, 7335, 5921, 1104, 4028]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "input_ids = inputs['input_ids'][0]\n",
    "input_ids[start_idx : end_idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a copper statue of Christ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[start_idx : end_idx + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['offset_mapping'][0][ctx_start][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer_token_idx(\n",
    "    ctx_start,\n",
    "    ctx_end,\n",
    "    ans_start_char,\n",
    "    ans_end_char,\n",
    "    offset):\n",
    "  \n",
    "  start_idx = 0\n",
    "  end_idx = 0\n",
    "\n",
    "  if offset[ctx_start][0] > ans_start_char or offset[ctx_end][1] < ans_end_char:\n",
    "    pass\n",
    "    # print(\"target is (0, 0)\")\n",
    "    # nothing else to do\n",
    "  else:\n",
    "    # find the start and end TOKEN positions\n",
    "\n",
    "    # the 'trick' is knowing what is in units of tokens and what is in\n",
    "    # units of characters\n",
    "\n",
    "    # recall: the offset_mapping contains the character positions of each token\n",
    "\n",
    "    i = ctx_start\n",
    "    for start_end_char in offset[ctx_start:]:\n",
    "      start, end = start_end_char\n",
    "      if start == ans_start_char:\n",
    "        start_idx = i\n",
    "        # don't break yet\n",
    "      \n",
    "      if end == ans_end_char:\n",
    "        end_idx = i\n",
    "        break\n",
    "\n",
    "      i += 1\n",
    "  return start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['offset_mapping'][0], inputs['offset_mapping'][1], inputs['offset_mapping'][2], inputs['offset_mapping'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([53, 17, 0, 0], [57, 21, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try it on all context windows\n",
    "# sometimes, the answer won't appear!\n",
    "\n",
    "start_idxs = []\n",
    "end_idxs = []\n",
    "\n",
    "for i, offset in enumerate(inputs['offset_mapping']):\n",
    "  # the final window may not be full size - can't assume 100\n",
    "  sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "  # find start + end of context (first 1 and last 1)\n",
    "  ctx_start = sequence_ids.index(1)\n",
    "  ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n",
    "\n",
    "  start_idx, end_idx = find_answer_token_idx(\n",
    "    ctx_start,\n",
    "    ctx_end,\n",
    "    ans_start_char,\n",
    "    ans_end_char,\n",
    "    offset)\n",
    "\n",
    "  start_idxs.append(start_idx)\n",
    "  end_idxs.append(end_idx)\n",
    "\n",
    "start_idxs, end_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In what city and state did Beyonce  grow up? \n",
      "272\n",
      " The album, Dangerously in Love  achieved what spot on the Billboard Top 100 chart?\n",
      "406\n",
      "Which song did Beyonce sing at the first couple's inaugural ball? \n",
      "463\n",
      "What event did Beyonc perform at one month after Obama's inauguration? \n",
      "548\n",
      "Where was the album released? \n",
      "565\n",
      "What movie influenced Beyonce towards empowerment themes? \n",
      "742\n"
     ]
    }
   ],
   "source": [
    "# some questions have leading and/or trailing whitespace\n",
    "for q in raw_datasets[\"train\"][\"question\"][:1000]:\n",
    "  if q.strip() != q:\n",
    "    print(q)\n",
    "    print(raw_datasets[\"train\"][\"question\"][:1000].index(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs['attention_mask'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f4190066117f',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'What is in front of the Notre Dame Main Building?',\n",
       " 'answers': {'text': ['a copper statue of Christ'], 'answer_start': [188]}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (0, 0)],\n",
       " [(0, 0),\n",
       "  (0, 4),\n",
       "  (5, 7),\n",
       "  (8, 10),\n",
       "  (11, 16),\n",
       "  (17, 19),\n",
       "  (20, 23),\n",
       "  (24, 29),\n",
       "  (30, 34),\n",
       "  (35, 39),\n",
       "  (40, 48),\n",
       "  (48, 49),\n",
       "  (0, 0),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0)]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 56), (56, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 251), (251, 254), (254, 256), (257, 259), (260, 262), (263, 264), (264, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 358), (358, 361), (361, 365), (366, 368), (369, 372), (373, 374), (374, 377), (377, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 446), (446, 449), (449, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 10), (11, 16), (17, 19), (20, 23), (24, 29), (30, 34), (35, 39), (40, 48), (48, 49), (0, 0), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 537), (537, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  # fruits = ['apple', 'banana', 'cherry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_mapping = inputs.pop(\"offset_mapping\")  # fruits.pop(1) = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 22182, 1193, 117, 1103, 1278, 1144, 170, 2336, 1959, 119, 1335, 4184, 1103, 4304, 4334, 112, 188, 2284, 10945, 1110, 170, 5404, 5921, 1104, 1103, 6567, 2090, 119, 13301, 1107, 1524, 1104, 1103, 4304, 4334, 1105, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 4749, 1122, 117, 1110, 170, 7335, 5921, 1104, 4028, 1114, 1739, 1146, 14089, 5591, 1114, 1103, 7051, 107, 159, 21462, 1566, 24930, 2508, 152, 1306, 3965, 107, 119, 5893, 1106, 1103, 4304, 4334, 1110, 1103, 19349, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 1104, 1103, 11373, 4641, 119, 13301, 1481, 1103, 171, 17506, 9538, 1110, 1103, 144, 10595, 2430, 117, 170, 14789, 1282, 1104, 8070, 1105, 9284, 119, 1135, 1110, 170, 16498, 1104, 1103, 176, 10595, 2430, 1120, 10111, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 102], [101, 1327, 1110, 1107, 1524, 1104, 1103, 10360, 8022, 4304, 4334, 136, 102, 20500, 117, 1699, 1187, 1103, 6567, 2090, 25153, 1193, 1691, 1106, 2216, 17666, 6397, 3786, 1573, 25422, 13149, 1107, 8109, 119, 1335, 1103, 1322, 1104, 1103, 1514, 2797, 113, 1105, 1107, 170, 2904, 1413, 1115, 8200, 1194, 124, 11739, 1105, 1103, 3487, 17917, 114, 117, 1110, 170, 3014, 117, 2030, 2576, 5921, 1104, 2090, 119, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)  # print(fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are ready to process (tokenize) the training data\n",
    "# (i.e. expand question+context pairs into question+smaller context windows)\n",
    "\n",
    "# Google used 384 for SQuAD\n",
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "def tokenize_fn_train(batch):\n",
    "  # some questions have leading and/or trailing whitespace\n",
    "  questions = [q.strip() for q in batch[\"question\"]]\n",
    "\n",
    "  # tokenize the data (with padding this time)\n",
    "  # since most contexts are long, we won't bother to pad per-minibatch\n",
    "  inputs = tokenizer(\n",
    "    questions,\n",
    "    batch[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    stride=stride,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    "  )\n",
    "\n",
    "  # we don't need these later so remove them\n",
    "  offset_mapping = inputs.pop(\"offset_mapping\")  # We do not understant why do not define????????????????????\n",
    "  orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "  answers = batch['answers']\n",
    "  start_idxs, end_idxs = [], []\n",
    "\n",
    "  # same loop as above\n",
    "  for i, offset in enumerate(offset_mapping):\n",
    "    sample_idx = orig_sample_idxs[i]\n",
    "    answer = answers[sample_idx]\n",
    "\n",
    "    ans_start_char = answer['answer_start'][0]\n",
    "    ans_end_char = ans_start_char + len(answer['text'][0])\n",
    "\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # find start + end of context (first 1 and last 1)\n",
    "    ctx_start = sequence_ids.index(1)\n",
    "    ctx_end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1\n",
    "\n",
    "    start_idx, end_idx = find_answer_token_idx(\n",
    "      ctx_start,\n",
    "      ctx_end,\n",
    "      ans_start_char,\n",
    "      ans_end_char,\n",
    "      offset)\n",
    "\n",
    "    start_idxs.append(start_idx)\n",
    "    end_idxs.append(end_idx)\n",
    "  \n",
    "  inputs[\"start_positions\"] = start_idxs\n",
    "  inputs[\"end_positions\"] = end_idxs\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'title', 'context', 'question', 'answers']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 88729)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "  tokenize_fn_train,\n",
    "  batched=True,\n",
    "  remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 88729\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be4db0acb8001400a502ec',\n",
       " 'title': 'Super_Bowl_50',\n",
       " 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 2410 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.',\n",
       " 'question': 'Which NFL team represented the AFC at Super Bowl 50?',\n",
       " 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       "  'answer_start': [177, 177, 177]}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: we'll keep these IDs for later\n",
    "raw_datasets[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the validation set differently\n",
    "# we won't need the targets since we will just compare with the original answer\n",
    "# also: overwrite offset_mapping with Nones in place of question\n",
    "def tokenize_fn_validation(batch):\n",
    "  # some questions have leading and/or trailing whitespace\n",
    "  questions = [q.strip() for q in batch[\"question\"]]\n",
    "\n",
    "  # tokenize the data (with padding this time)\n",
    "  # since most contexts are long, we won't bother to pad per-minibatch\n",
    "  inputs = tokenizer(\n",
    "    questions,\n",
    "    batch[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    stride=stride,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    "  )\n",
    "\n",
    "  # we don't need these later so remove them\n",
    "  orig_sample_idxs = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "  sample_ids = []\n",
    "\n",
    "  # rewrite offset mapping by replacing question tuples with None\n",
    "  # this will be helpful later on when we compute metrics\n",
    "  for i in range(len(inputs[\"input_ids\"])):  # i : 0, 1, 2, 3\n",
    "    sample_idx = orig_sample_idxs[i]\n",
    "    sample_ids.append(batch['id'][sample_idx])\n",
    "\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "    offset = inputs[\"offset_mapping\"][i]\n",
    "    inputs[\"offset_mapping\"][i] = [\n",
    "      x if sequence_ids[j] == 1 else None for j, x in enumerate(offset)]\n",
    "    \n",
    "  inputs['sample_id'] = sample_ids\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset = inputs['offset_mapping'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs['offset_mapping'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['overflow_to_sample_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_sample_idxs = [0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10570, 10822)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "  tokenize_fn_validation,\n",
    "  batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nilofar\\AppData\\Local\\Temp\\ipykernel_14492\\3071101299.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad\")\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 66.66666666666667, 'f1': 83.33333333333333}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_answers = [\n",
    "  {'id': '1', 'prediction_text': 'Albert Einstein'},\n",
    "  {'id': '2', 'prediction_text': 'physicist'},\n",
    "  {'id': '3', 'prediction_text': 'general relativity'},\n",
    "]\n",
    "true_answers = [\n",
    "  {'id': '1', 'answers': {'text': ['Albert Einstein'], 'answer_start': [100]}},\n",
    "  {'id': '2', 'answers': {'text': ['physicist'], 'answer_start': [100]}},\n",
    "  {'id': '3', 'answers': {'text': ['special relativity'], 'answer_start': [100]}},\n",
    "]\n",
    "\n",
    "# id and answer_start seem superfluous but you'll get an error if not included\n",
    "# exercise: remove them (one at a time) and see!\n",
    "metric.compute(predictions=predicted_answers, references=true_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next problem: how to go from logits to prediction text?\n",
    "# to make it easier, let's work on an already-trained question-answering model\n",
    "small_validation_dataset = raw_datasets[\"validation\"].select(range(100))\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "\n",
    "# temporarily assign tokenizer2 to tokenizer since it's used as a global\n",
    "# in tokenize_fn_validation\n",
    "old_tokenizer = tokenizer\n",
    "tokenizer = tokenizer2\n",
    "\n",
    "small_validation_processed = small_validation_dataset.map(\n",
    "    tokenize_fn_validation,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "\n",
    "\n",
    "len(small_validation_dataset), len(small_validation_processed)\n",
    "# change it back\n",
    "tokenizer = old_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'offset_mapping': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n",
       " 'sample_id': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_validation_processed.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_validation_dataset), len(small_validation_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Super_Bowl_50'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"validation\"][20]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model outputs\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# the trained model doesn't use these columns\n",
    "small_model_inputs = small_validation_processed.remove_columns(\n",
    "  [\"sample_id\", \"offset_mapping\"])\n",
    "small_model_inputs.set_format(\"torch\")\n",
    "\n",
    "# get gpu device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# move tensors to gpu device\n",
    "small_model_inputs_gpu = {\n",
    "  k: small_model_inputs[k].to(device) for k in small_model_inputs.column_names\n",
    "}\n",
    "\n",
    "# download the model\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "  trained_checkpoint).to(device)\n",
    "\n",
    "# get the model outputs\n",
    "with torch.no_grad():\n",
    "  outputs = trained_model(**small_model_inputs_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ -2.2607,  -5.1783,  -5.2709,  ...,  -9.5243,  -9.5183,  -9.5288],\n",
       "        [ -2.5961,  -5.5482,  -5.5313,  ...,  -9.9598,  -9.9533,  -9.9860],\n",
       "        [ -3.7127,  -7.1848,  -8.5388,  ..., -11.6557, -11.6571, -11.6505],\n",
       "        ...,\n",
       "        [ -2.0260,  -4.4167,  -4.4980,  ...,  -8.1479,  -8.1530,  -8.1760],\n",
       "        [ -4.1553,  -5.8304,  -7.1643,  ..., -10.5255, -10.5251, -10.4890],\n",
       "        [ -3.2000,  -5.8162,  -6.7249,  ...,  -9.4935,  -9.5038,  -9.4871]]), end_logits=tensor([[ -0.7353,  -4.9236,  -5.1048,  ...,  -8.8734,  -8.8915,  -8.8550],\n",
       "        [ -1.3056,  -5.3870,  -5.4945,  ...,  -9.4895,  -9.5039,  -9.4958],\n",
       "        [ -2.7649,  -7.2201,  -9.0916,  ..., -11.3106, -11.3414, -11.2702],\n",
       "        ...,\n",
       "        [ -0.0768,  -4.8210,  -4.4374,  ...,  -8.0483,  -8.0502,  -7.9903],\n",
       "        [ -2.7347,  -5.3650,  -7.2549,  ..., -10.0498, -10.0661,  -9.9886],\n",
       "        [ -1.0991,  -4.2569,  -6.1267,  ...,  -8.6882,  -8.6889,  -8.6272]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 384])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"start_logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -2.2607,  -5.1783,  -5.2709,  ...,  -9.5243,  -9.5183,  -9.5288],\n",
       "        [ -2.5961,  -5.5482,  -5.5313,  ...,  -9.9598,  -9.9533,  -9.9860],\n",
       "        [ -3.7127,  -7.1848,  -8.5388,  ..., -11.6557, -11.6571, -11.6505],\n",
       "        ...,\n",
       "        [ -2.0260,  -4.4167,  -4.4980,  ...,  -8.1479,  -8.1530,  -8.1760],\n",
       "        [ -4.1553,  -5.8304,  -7.1643,  ..., -10.5255, -10.5251, -10.4890],\n",
       "        [ -3.2000,  -5.8162,  -6.7249,  ...,  -9.4935,  -9.5038,  -9.4871]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"start_logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 384])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"end_logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs[\"end_logits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.2607286,  -5.178324 ,  -5.270891 , ...,  -9.524325 ,\n",
       "         -9.518305 ,  -9.528757 ],\n",
       "       [ -2.5960746,  -5.5482106,  -5.531334 , ...,  -9.959751 ,\n",
       "         -9.953276 ,  -9.986047 ],\n",
       "       [ -3.7127328,  -7.184835 ,  -8.538828 , ..., -11.655701 ,\n",
       "        -11.6571455, -11.650536 ],\n",
       "       ...,\n",
       "       [ -2.026028 ,  -4.4166594,  -4.4980016, ...,  -8.147888 ,\n",
       "         -8.153048 ,  -8.175961 ],\n",
       "       [ -4.155297 ,  -5.83042  ,  -7.164263 , ..., -10.525518 ,\n",
       "        -10.525085 , -10.489031 ],\n",
       "       [ -3.2000246,  -5.816207 ,  -6.7249413, ...,  -9.493472 ,\n",
       "         -9.503824 ,  -9.487104 ]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['56be4db0acb8001400a502ec',\n",
       " '56be4db0acb8001400a502ed',\n",
       " '56be4db0acb8001400a502ee',\n",
       " '56be4db0acb8001400a502ef',\n",
       " '56be4db0acb8001400a502f0']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_validation_processed['sample_id'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'sample_id'],\n",
       "    num_rows: 10822\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10822"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_dataset['sample_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['56bea9923aeaaa14008c91bb',\n",
       " '56beace93aeaaa14008c91df',\n",
       " '56beace93aeaaa14008c91e0',\n",
       " '56beace93aeaaa14008c91e1',\n",
       " '56beace93aeaaa14008c91e2',\n",
       " '56beace93aeaaa14008c91e3',\n",
       " '56bf10f43aeaaa14008c94fd',\n",
       " '56bf10f43aeaaa14008c94fe',\n",
       " '56bf10f43aeaaa14008c94ff',\n",
       " '56bf10f43aeaaa14008c9500',\n",
       " '56bf10f43aeaaa14008c9501',\n",
       " '56d20362e7d4791d009025e8',\n",
       " '56d20362e7d4791d009025e9',\n",
       " '56d20362e7d4791d009025ea',\n",
       " '56d20362e7d4791d009025eb',\n",
       " '56d600e31c85041400946eae',\n",
       " '56d600e31c85041400946eb0',\n",
       " '56d600e31c85041400946eb1',\n",
       " '56d9895ddc89441400fdb50e',\n",
       " '56d9895ddc89441400fdb510']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset['sample_id'][10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'56be4db0acb8001400a502ed'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset['sample_id'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'56be4db0acb8001400a502ec'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset['sample_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10570"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(validation_dataset['sample_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['56bea9923aeaaa14008c91bb',\n",
       " '56beace93aeaaa14008c91df',\n",
       " '56beace93aeaaa14008c91e0',\n",
       " '56beace93aeaaa14008c91e1',\n",
       " '56beace93aeaaa14008c91e2']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset['sample_id'][10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['56bea9923aeaaa14008c91bb',\n",
       " '56beace93aeaaa14008c91df',\n",
       " '56beace93aeaaa14008c91e0',\n",
       " '56beace93aeaaa14008c91e1',\n",
       " '56beace93aeaaa14008c91e2']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_validation_processed['sample_id'][10:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 56bea9923aeaaa14008c91bb\n",
      "1 56beace93aeaaa14008c91df\n",
      "2 56beace93aeaaa14008c91e0\n",
      "3 56beace93aeaaa14008c91e1\n",
      "4 56beace93aeaaa14008c91e2\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(small_validation_processed['sample_id'][10:15]):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_id2idxs = {key: value}  sample_id2idxs[keys] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id2idxs = {}  # {\"56bea9923aeaaa14008c91bb\": [0, 10400], \"56beace93aeaaa14008c91df\": [1]}\n",
    "for i, id_ in enumerate(small_validation_processed['sample_id']):\n",
    "  if id_ not in sample_id2idxs:\n",
    "    sample_id2idxs[id_] = [i]\n",
    "  else:\n",
    "    print(\"here\")\n",
    "    sample_id2idxs[id_].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: {'56be4db0acb8001400a502ec': [0, 1, 2, 3], ...}\n",
    "sample_id2idxs = {}\n",
    "for i, id_ in enumerate(small_validation_processed['sample_id']):\n",
    "  if id_ not in sample_id2idxs:\n",
    "    sample_id2idxs[id_] = [i]\n",
    "  else:\n",
    "    print(\"here\")\n",
    "    sample_id2idxs[id_].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 384), (100, 384))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits.shape, end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 46,  57,  47,  38,  39,  58,  50,  43,  45,  54,  56,  49,  13,\n",
       "        42,  40,  35,  27,  31,  48,  41,  53,  44,  37,  59,  78,  15,\n",
       "         0,  52,  24,  65,  81,  70,  18,  51,  55,  26,  69,  29,  28,\n",
       "        75,  61,  64,  23,  36,  32,  11, 101,  62,  66,  34,  95,  30,\n",
       "        63,  21,  19,  20,  17,  14,  22,  33,  68,  87, 171,  12,  76,\n",
       "        71,  73,  92, 110,  84, 151,   1,  74,   2,   6,  16,  80,  79,\n",
       "       105,  98,  10,  96, 136, 169, 106, 100,  93, 165,  67, 109,   8,\n",
       "        90,   3, 115,  60,   5,  97,   7, 103, 102,  86,  72, 111,  89,\n",
       "       108,   4,  88,  25, 132,  77, 123, 150, 124, 153,  83, 118,  82,\n",
       "        85, 107, 114, 143, 164, 137, 130, 166, 159, 131,  91,   9, 144,\n",
       "       139, 160,  94, 141, 128, 112, 134, 152, 170, 154, 117, 127, 104,\n",
       "       140, 157, 155, 133, 145, 119, 162, 138, 135, 156, 167, 168, 126,\n",
       "       148, 163, 161, 116,  99, 120, 142, 158, 125, 146, 113, 121, 147,\n",
       "       149, 129, 122, 311, 312, 304, 309, 313, 310, 300, 307, 316, 308,\n",
       "       314, 306, 317, 320, 319, 321, 291, 318, 301, 305, 287, 270, 315,\n",
       "       295, 289, 294, 251, 333, 303, 269, 299, 274, 265, 298, 176, 175,\n",
       "       338, 292, 323, 322, 290, 252, 296, 229, 177, 302, 297, 186, 245,\n",
       "       250, 283, 174, 256, 337, 266, 190, 293, 286, 264, 288, 331, 327,\n",
       "       234, 237, 227, 284, 255, 326, 276, 272, 233, 346, 191, 230, 218,\n",
       "       232, 179, 285, 273, 173, 187, 239, 332, 172, 267, 329, 238, 253,\n",
       "       334, 214, 192, 325, 278, 350, 259, 281, 268, 185, 254, 271, 279,\n",
       "       342, 345, 343, 181, 335, 183, 189, 260, 341, 275, 178, 228, 210,\n",
       "       324, 277, 212, 348, 209, 336, 261, 240, 249, 246, 194, 257, 182,\n",
       "       377, 258, 196, 195, 248, 188, 339, 197, 213, 378, 263, 208, 201,\n",
       "       205, 200, 225, 211, 282, 236, 204, 347, 203, 262, 223, 193, 330,\n",
       "       199, 202, 349, 184, 180, 351, 340, 226, 224, 243, 217, 372, 244,\n",
       "       241, 207, 235, 344, 215, 367, 247, 368, 382, 352, 379, 353, 221,\n",
       "       376, 231, 380, 220, 371, 366, 242, 219, 381, 206, 375, 369, 216,\n",
       "       198, 355, 383, 280, 328, 222, 358, 373, 357, 363, 356, 374, 354,\n",
       "       359, 365, 370, 364, 362, 361, 360], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reminder of how to find indices with the largest values\n",
    "(-start_logits[0]).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.694451  ,  9.803686  ,  4.4599786 ,  4.400489  ,  2.9437866 ,\n",
       "        2.7017422 ,  2.012652  ,  1.5780808 ,  0.52237856,  0.02074176,\n",
       "       -0.02801922, -0.04970397, -0.38572356, -0.6945309 , -0.7979434 ,\n",
       "       -0.86779773, -0.8722001 , -1.3516843 , -1.3703636 , -1.3878787 ,\n",
       "       -1.5135026 , -1.7355411 , -1.8827026 , -1.8932847 , -1.9078901 ,\n",
       "       -1.9304947 , -2.2607286 , -2.2983828 , -2.3069277 , -2.5027347 ,\n",
       "       -2.5100536 , -2.5308342 , -2.5399885 , -2.6718073 , -2.7323494 ,\n",
       "       -2.7710114 , -2.7713616 , -2.952129  , -3.0604606 , -3.170597  ,\n",
       "       -3.2045417 , -3.569333  , -3.5797982 , -3.6668777 , -3.7250547 ,\n",
       "       -3.7498534 , -3.7632098 , -3.9968114 , -4.011324  , -4.0687957 ,\n",
       "       -4.0944786 , -4.19547   , -4.238307  , -4.332359  , -4.35241   ,\n",
       "       -4.3879576 , -4.388604  , -4.396608  , -4.6790495 , -4.7030234 ,\n",
       "       -4.77575   , -4.777807  , -4.7882137 , -4.7882376 , -4.822118  ,\n",
       "       -4.8725348 , -4.884929  , -4.898141  , -5.072089  , -5.107869  ,\n",
       "       -5.148633  , -5.178324  , -5.191201  , -5.270891  , -5.3146367 ,\n",
       "       -5.3770666 , -5.5316434 , -5.5711217 , -5.649811  , -5.6613007 ,\n",
       "       -5.6778693 , -5.698604  , -5.7515626 , -5.842641  , -5.862148  ,\n",
       "       -5.9076815 , -5.9093714 , -5.944444  , -5.9968905 , -6.0058203 ,\n",
       "       -6.0470276 , -6.0640063 , -6.0858717 , -6.1005545 , -6.224171  ,\n",
       "       -6.267088  , -6.2752852 , -6.3032913 , -6.3134108 , -6.3287973 ,\n",
       "       -6.330685  , -6.4014792 , -6.4204807 , -6.436103  , -6.4374037 ,\n",
       "       -6.4507074 , -6.5314255 , -6.5530386 , -6.563015  , -6.66879   ,\n",
       "       -6.7266874 , -6.742626  , -6.7442894 , -6.7458506 , -6.810868  ,\n",
       "       -6.921628  , -6.949293  , -6.990761  , -6.998742  , -7.162259  ,\n",
       "       -7.1673503 , -7.175652  , -7.180557  , -7.1967764 , -7.2435923 ,\n",
       "       -7.2733355 , -7.2769456 , -7.300214  , -7.3009276 , -7.3218546 ,\n",
       "       -7.3976755 , -7.4330273 , -7.5248423 , -7.5272284 , -7.532271  ,\n",
       "       -7.5737076 , -7.59821   , -7.6580915 , -7.6597023 , -7.7155733 ,\n",
       "       -7.7183986 , -7.740014  , -7.7546453 , -7.885642  , -7.893759  ,\n",
       "       -7.8943686 , -7.9380317 , -7.9581985 , -8.03751   , -8.059216  ,\n",
       "       -8.063906  , -8.104628  , -8.117329  , -8.138481  , -8.145773  ,\n",
       "       -8.221216  , -8.274244  , -8.292221  , -8.293204  , -8.316826  ,\n",
       "       -8.317493  , -8.333995  , -8.466507  , -8.500577  , -8.502031  ,\n",
       "       -8.591032  , -8.612466  , -8.621489  , -8.625208  , -8.625452  ,\n",
       "       -8.814257  , -9.039049  , -9.421054  , -9.434788  , -9.441465  ,\n",
       "       -9.44437   , -9.449843  , -9.450167  , -9.450434  , -9.45104   ,\n",
       "       -9.452059  , -9.452422  , -9.452511  , -9.454128  , -9.456524  ,\n",
       "       -9.458102  , -9.458338  , -9.458435  , -9.45875   , -9.460867  ,\n",
       "       -9.4620695 , -9.462508  , -9.46294   , -9.463791  , -9.463888  ,\n",
       "       -9.465527  , -9.466446  , -9.468559  , -9.468852  , -9.468952  ,\n",
       "       -9.469486  , -9.469515  , -9.47006   , -9.470961  , -9.471134  ,\n",
       "       -9.471281  , -9.471354  , -9.4717865 , -9.4718895 , -9.472044  ,\n",
       "       -9.472683  , -9.472958  , -9.473705  , -9.475197  , -9.476604  ,\n",
       "       -9.476763  , -9.47678   , -9.477406  , -9.478306  , -9.478869  ,\n",
       "       -9.479746  , -9.479916  , -9.480066  , -9.480648  , -9.481186  ,\n",
       "       -9.481466  , -9.481517  , -9.4815235 , -9.481607  , -9.481789  ,\n",
       "       -9.482155  , -9.482843  , -9.48341   , -9.4845705 , -9.484707  ,\n",
       "       -9.484895  , -9.485168  , -9.485298  , -9.485716  , -9.485956  ,\n",
       "       -9.486801  , -9.486967  , -9.487631  , -9.487726  , -9.487911  ,\n",
       "       -9.487989  , -9.4882965 , -9.488396  , -9.48859   , -9.488799  ,\n",
       "       -9.488955  , -9.489097  , -9.489484  , -9.489672  , -9.489996  ,\n",
       "       -9.490801  , -9.4908285 , -9.4908695 , -9.490896  , -9.491137  ,\n",
       "       -9.491253  , -9.491707  , -9.492024  , -9.492165  , -9.492535  ,\n",
       "       -9.492814  , -9.493554  , -9.493632  , -9.493645  , -9.494001  ,\n",
       "       -9.494024  , -9.494388  , -9.495201  , -9.49542   , -9.496533  ,\n",
       "       -9.4966    , -9.497129  , -9.497523  , -9.497554  , -9.497576  ,\n",
       "       -9.497602  , -9.497613  , -9.4980135 , -9.498119  , -9.498622  ,\n",
       "       -9.49864   , -9.498762  , -9.499582  , -9.50001   , -9.50034   ,\n",
       "       -9.500418  , -9.500574  , -9.500973  , -9.500995  , -9.501165  ,\n",
       "       -9.501216  , -9.50127   , -9.50143   , -9.501499  , -9.501879  ,\n",
       "       -9.502006  , -9.502519  , -9.502683  , -9.503559  , -9.5036335 ,\n",
       "       -9.503656  , -9.504164  , -9.504545  , -9.505045  , -9.505529  ,\n",
       "       -9.506167  , -9.506636  , -9.50667   , -9.506752  , -9.506826  ,\n",
       "       -9.507116  , -9.507551  , -9.507726  , -9.507986  , -9.508047  ,\n",
       "       -9.50808   , -9.508731  , -9.508766  , -9.50886   , -9.509272  ,\n",
       "       -9.50951   , -9.509527  , -9.51082   , -9.511486  , -9.512029  ,\n",
       "       -9.512419  , -9.512815  , -9.51313   , -9.513149  , -9.513187  ,\n",
       "       -9.513235  , -9.513384  , -9.514926  , -9.5154295 , -9.515565  ,\n",
       "       -9.516184  , -9.516241  , -9.516339  , -9.516375  , -9.516655  ,\n",
       "       -9.517014  , -9.518305  , -9.518465  , -9.518789  , -9.519087  ,\n",
       "       -9.520797  , -9.520998  , -9.521187  , -9.521225  , -9.522537  ,\n",
       "       -9.5227375 , -9.522882  , -9.5232315 , -9.523891  , -9.524325  ,\n",
       "       -9.525372  , -9.526033  , -9.527027  , -9.528467  , -9.528528  ,\n",
       "       -9.528619  , -9.528757  , -9.529456  , -9.529707  , -9.531367  ,\n",
       "       -9.532364  , -9.532742  , -9.535534  , -9.536642  , -9.5377035 ,\n",
       "       -9.53866   , -9.539051  , -9.539336  , -9.540269  , -9.541702  ,\n",
       "       -9.548321  , -9.554476  , -9.557684  , -9.567398  ], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits[0][(-start_logits[0]).argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " [0, 5],\n",
       " [6, 10],\n",
       " [11, 13],\n",
       " [14, 17],\n",
       " [18, 20],\n",
       " [21, 29],\n",
       " [30, 38],\n",
       " [39, 43],\n",
       " [44, 46],\n",
       " [47, 56],\n",
       " [57, 60],\n",
       " [61, 69],\n",
       " [70, 72],\n",
       " [73, 76],\n",
       " [77, 85],\n",
       " [86, 94],\n",
       " [95, 101],\n",
       " [102, 103],\n",
       " [103, 106],\n",
       " [106, 107],\n",
       " [108, 111],\n",
       " [112, 115],\n",
       " [116, 120],\n",
       " [121, 127],\n",
       " [127, 128],\n",
       " [129, 132],\n",
       " [133, 141],\n",
       " [142, 150],\n",
       " [151, 161],\n",
       " [162, 163],\n",
       " [163, 166],\n",
       " [166, 167],\n",
       " [168, 176],\n",
       " [177, 183],\n",
       " [184, 191],\n",
       " [192, 200],\n",
       " [201, 204],\n",
       " [205, 213],\n",
       " [214, 222],\n",
       " [223, 233],\n",
       " [234, 235],\n",
       " [235, 238],\n",
       " [238, 239],\n",
       " [240, 248],\n",
       " [249, 257],\n",
       " [258, 266],\n",
       " [267, 269],\n",
       " [269, 270],\n",
       " [270, 272],\n",
       " [273, 275],\n",
       " [276, 280],\n",
       " [281, 286],\n",
       " [287, 292],\n",
       " [293, 298],\n",
       " [299, 303],\n",
       " [304, 309],\n",
       " [309, 310],\n",
       " [311, 314],\n",
       " [315, 319],\n",
       " [320, 323],\n",
       " [324, 330],\n",
       " [331, 333],\n",
       " [334, 342],\n",
       " [343, 344],\n",
       " [344, 345],\n",
       " [346, 350],\n",
       " [350, 351],\n",
       " [352, 354],\n",
       " [355, 359],\n",
       " [359, 360],\n",
       " [360, 361],\n",
       " [362, 369],\n",
       " [370, 372],\n",
       " [373, 376],\n",
       " [377, 380],\n",
       " [381, 390],\n",
       " [391, 394],\n",
       " [395, 399],\n",
       " [400, 402],\n",
       " [403, 408],\n",
       " [409, 414],\n",
       " [414, 415],\n",
       " [416, 426],\n",
       " [426, 427],\n",
       " [428, 430],\n",
       " [431, 435],\n",
       " [436, 439],\n",
       " [440, 443],\n",
       " [444, 448],\n",
       " [449, 454],\n",
       " [455, 459],\n",
       " [459, 460],\n",
       " [461, 464],\n",
       " [465, 471],\n",
       " [472, 482],\n",
       " [483, 486],\n",
       " [487, 488],\n",
       " [488, 494],\n",
       " [495, 506],\n",
       " [506, 507],\n",
       " [508, 512],\n",
       " [513, 520],\n",
       " [521, 525],\n",
       " [525, 526],\n",
       " [526, 532],\n",
       " [533, 544],\n",
       " [544, 545],\n",
       " [546, 548],\n",
       " [549, 553],\n",
       " [554, 556],\n",
       " [557, 568],\n",
       " [569, 571],\n",
       " [571, 573],\n",
       " [573, 579],\n",
       " [580, 583],\n",
       " [584, 593],\n",
       " [594, 596],\n",
       " [597, 603],\n",
       " [604, 608],\n",
       " [609, 614],\n",
       " [615, 619],\n",
       " [620, 624],\n",
       " [625, 629],\n",
       " [630, 635],\n",
       " [636, 637],\n",
       " [637, 640],\n",
       " [640, 644],\n",
       " [645, 646],\n",
       " [646, 651],\n",
       " [652, 657],\n",
       " [658, 661],\n",
       " [662, 666],\n",
       " [667, 672],\n",
       " [673, 677],\n",
       " [678, 682],\n",
       " [683, 688],\n",
       " [689, 691],\n",
       " [692, 693],\n",
       " [693, 698],\n",
       " [699, 703],\n",
       " [704, 705],\n",
       " [705, 706],\n",
       " [706, 707],\n",
       " [707, 708],\n",
       " [709, 711],\n",
       " [712, 716],\n",
       " [717, 720],\n",
       " [721, 725],\n",
       " [726, 731],\n",
       " [732, 743],\n",
       " [744, 751],\n",
       " [752, 755],\n",
       " [756, 762],\n",
       " [763, 764],\n",
       " [764, 767],\n",
       " [767, 771],\n",
       " [772, 774],\n",
       " [774, 775],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reminder: in offset_mapping we store None everywhere except the context window\n",
    "# in the context window we store tuples for each token containing:\n",
    "# (start_character_position, end_character_position)\n",
    "small_validation_processed['offset_mapping'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_largest = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "# we are looping through the original (untokenized) dataset\n",
    "# because we need to grab the answer from the original string context\n",
    "for sample in small_validation_dataset:\n",
    "  sample_id = sample['id']\n",
    "  context = sample['context']\n",
    "\n",
    "  # update these as we loop through candidate answers\n",
    "  best_score = float('-inf')\n",
    "  best_answer = None\n",
    "\n",
    "  # now loop through the *expanded* input samples (fixed size context windows)\n",
    "  # from here we will pick the highest probability start/end combination\n",
    "  for idx in sample_id2idxs[sample_id]:\n",
    "    start_logit = start_logits[idx] # (384,) vector\n",
    "    end_logit = end_logits[idx] # (384,) vector\n",
    "    offsets = small_validation_processed[idx]['offset_mapping']\n",
    "\n",
    "    start_indices = (-start_logit).argsort()\n",
    "    end_indices = (-end_logit).argsort()\n",
    "\n",
    "    for start_idx in start_indices[:n_largest]:\n",
    "      for end_idx in end_indices[:n_largest]:\n",
    "\n",
    "        # skip answers not contained in context window\n",
    "        # recall: we set entries not pertaining to context to None earlier\n",
    "        if offsets[start_idx] is None or offsets[end_idx] is None:\n",
    "          continue\n",
    "        \n",
    "        # skip answers where end < start\n",
    "        if end_idx < start_idx:\n",
    "          continue\n",
    "        \n",
    "        # skip answers that are too long\n",
    "        if end_idx - start_idx + 1 > max_answer_length:\n",
    "          continue\n",
    "        \n",
    "        # see theory lecture for score calculation\n",
    "        score = start_logit[start_idx] + end_logit[end_idx]\n",
    "        if score > best_score:\n",
    "          best_score = score\n",
    "\n",
    "          # find positions of start and end characters\n",
    "          # recall: offsets contains tuples for each token:\n",
    "          # (start_char, end_char)\n",
    "          first_ch = offsets[start_idx][0]\n",
    "          last_ch = offsets[end_idx][1]\n",
    "\n",
    "          best_answer = context[first_ch:last_ch]\n",
    "\n",
    "  # save best answer\n",
    "  predicted_answers.append({'id': sample_id, 'prediction_text': best_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_validation_dataset['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now test it!\n",
    "\n",
    "true_answers = [\n",
    "  {'id': x['id'], 'answers': x['answers']} for x in small_validation_dataset\n",
    "]\n",
    "metric.compute(predictions=predicted_answers, references=true_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's define a full compute_metrics function\n",
    "# note: this will NOT be called from the trainer\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, processed_dataset, orig_dataset):\n",
    "  # map sample_id ('56be4db0acb8001400a502ec') to row indices of processed data\n",
    "  sample_id2idxs = {}\n",
    "  for i, id_ in enumerate(processed_dataset['sample_id']):\n",
    "    if id_ not in sample_id2idxs:\n",
    "      sample_id2idxs[id_] = [i]\n",
    "    else:\n",
    "      sample_id2idxs[id_].append(i)\n",
    "\n",
    "  predicted_answers = []\n",
    "  for sample in tqdm(orig_dataset):\n",
    "\n",
    "    sample_id = sample['id']\n",
    "    context = sample['context']\n",
    "\n",
    "    # update these as we loop through candidate answers\n",
    "    best_score = float('-inf')\n",
    "    best_answer = None\n",
    "\n",
    "    # now loop through the *expanded* input samples (fixed size context windows)\n",
    "    # from here we will pick the highest probability start/end combination\n",
    "    for idx in sample_id2idxs[sample_id]:\n",
    "      start_logit = start_logits[idx] # (T,) vector\n",
    "      end_logit = end_logits[idx] # (T,) vector\n",
    "\n",
    "      # note: do NOT do the reverse: ['offset_mapping'][idx]\n",
    "      offsets = processed_dataset[idx]['offset_mapping']\n",
    "\n",
    "      start_indices = (-start_logit).argsort()\n",
    "      end_indices = (-end_logit).argsort()\n",
    "\n",
    "      for start_idx in start_indices[:n_largest]:\n",
    "        for end_idx in end_indices[:n_largest]:\n",
    "\n",
    "          # skip answers not contained in context window\n",
    "          # recall: we set entries not pertaining to context to None earlier\n",
    "          if offsets[start_idx] is None or offsets[end_idx] is None:\n",
    "            continue\n",
    "          \n",
    "          # skip answers where end < start\n",
    "          if end_idx < start_idx:\n",
    "            continue\n",
    "          \n",
    "          # skip answers that are too long\n",
    "          if end_idx - start_idx + 1 > max_answer_length:\n",
    "            continue\n",
    "          \n",
    "          # see theory lecture for score calculation\n",
    "          score = start_logit[start_idx] + end_logit[end_idx]\n",
    "          if score > best_score:\n",
    "            best_score = score\n",
    "\n",
    "            # find positions of start and end characters\n",
    "            # recall: offsets contains tuples for each token:\n",
    "            # (start_char, end_char)\n",
    "            first_ch = offsets[start_idx][0]\n",
    "            last_ch = offsets[end_idx][1]\n",
    "\n",
    "            best_answer = context[first_ch:last_ch]\n",
    "\n",
    "    # save best answer\n",
    "    predicted_answers.append({'id': sample_id, 'prediction_text': best_answer})\n",
    "  \n",
    "  # compute the metrics\n",
    "  true_answers = [\n",
    "    {'id': x['id'], 'answers': x['answers']} for x in orig_dataset\n",
    "  ]\n",
    "  return metric.compute(predictions=predicted_answers, references=true_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 601.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 83.0, 'f1': 88.25000000000004}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run our function on the same mini dataset as before\n",
    "compute_metrics(\n",
    "    start_logits,\n",
    "    end_logits,\n",
    "    small_validation_processed,\n",
    "    small_validation_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# now load the model we want to fine-tune\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nilofar\\Desktop\\Q-Answering_Lazyprogrammer\\QA1 copy.ipynb Cell 82\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#Y131sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m TrainingArguments\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#Y131sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#Y131sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mfinetuned-squad\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#Y131sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mno\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#Y131sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     save_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#Y131sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m2e-5\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#Y131sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#Y131sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#Y131sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     fp16\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nilofar/Desktop/Q-Answering_Lazyprogrammer/QA1%20copy.ipynb#Y131sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[1;32m<string>:114\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1405\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1399\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[0;32m   1400\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1402\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1403\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1405\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1406\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1407\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1408\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1409\u001b[0m ):\n\u001b[0;32m   1410\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1411\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1412\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA or NPU devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1413\u001b[0m     )\n\u001b[0;32m   1415\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1416\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1417\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1422\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1423\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1852\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1848\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1850\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 1852\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[0;32m     55\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1767\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1765\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available(min_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m0.20.1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   1768\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1769\u001b[0m         )\n\u001b[0;32m   1770\u001b[0m     AcceleratorState\u001b[39m.\u001b[39m_reset_state(reset_partial_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1771\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"finetuned-squad\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
